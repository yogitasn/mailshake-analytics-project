{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9646918b-50f0-4870-ac46-4892067ee656",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/28 00:47:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/28 00:47:44 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------------------+--------------------+----------+--------+------------+--------------------+--------------------+-------------+-------------------+--------------------+--------------------+--------------------+\n",
      "|  object|     id|               title|             created|isArchived|isPaused|wizardStatus|            messages|                 url|sender.object|          sender.id| sender.emailAddress|     sender.fromName|      sender.created|\n",
      "+--------+-------+--------------------+--------------------+----------+--------+------------+--------------------+--------------------+-------------+-------------------+--------------------+--------------------+--------------------+\n",
      "|campaign|1488098|2025_12_Sanghamit...|2025-12-19T14:32:...|     false|   false|  InProgress|[{4065551, false,...|https://mailshake...|       sender|164486-178439-false|palsanghamitra22@...|     Sanghamitra Pal|2025-12-19T14:23:...|\n",
      "|campaign|1488093|2025_12_Vansh_Gup...|2025-12-19T14:12:...|     false|   false|  InProgress|[{4065536, false,...|https://mailshake...|       sender|164485-178438-false|vanshguptainca@gm...|         Vansh Gupta|2025-12-19T14:11:...|\n",
      "|campaign|1458402|2025_09_Avinash_P...|2025-09-09T08:10:...|     false|    true|  InProgress|[{3994937, false,...|https://mailshake...|       sender|159285-173136-false|pashamavinashredd...|Avinash Reddy Pasham|2025-09-09T08:09:...|\n",
      "|campaign|1447546|Cold_Emailing_Meg...|2025-08-06T10:11:...|     false|   false|  InProgress|[{3967985, false,...|https://mailshake...|       sender|157092-170911-false|megha.mh10@gmail.com|          Megha Hole|2025-08-06T09:34:...|\n",
      "|campaign|1447134|2025_08_Lekhya_Sa...|2025-08-05T11:22:...|     false|    true|  InProgress|[{3966984, false,...|https://mailshake...|       sender|157006-170827-false|sakelekhya@gmail.com|         Lekhya Sake|2025-08-05T11:13:...|\n",
      "|campaign|1425545|02_06_2025_Recrui...|2025-06-02T15:28:...|     false|    true|  InProgress|[{3914469, false,...|https://mailshake...|       sender|153577-167366-false|dhyeydeacademy@gm...|         Dhyey Patel|2025-06-02T15:26:...|\n",
      "|campaign|1425327|2025_06_Megha_hol...|2025-06-02T09:50:...|     false|   false|  InProgress|[{3914483, false,...|https://mailshake...|       sender|153567-167356-false|meghahole109@gmai...|          Megha Hole|2025-06-02T09:49:...|\n",
      "|campaign|1419805|          Brian Rowe|2025-05-19T15:28:...|     false|    true|  InProgress|[{3900330, false,...|https://mailshake...|       sender|152559-166330-false| browe7379@gmail.com|              B Rowe|2025-05-19T15:26:...|\n",
      "|campaign|1410676|2025_04_McLain_Cr...|2025-04-23T13:58:...|     false|    true|  InProgress|[{3876605, false,...|https://mailshake...|       sender|150635-164379-false|mclaincronin.mc@g...|       McLain Cronin|2025-04-17T10:14:...|\n",
      "+--------+-------+--------------------+--------------------+----------+--------+------------+--------------------+--------------------+-------------+-------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Create Spark session with S3 configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"JupyterS3\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/opt/spark/jars/hadoop-aws-3.3.4.jar:/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar\") \\\n",
    "    .config(\"spark.executor.extraClassPath\", \"/opt/spark/jars/hadoop-aws-3.3.4.jar:/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configure S3\n",
    "aws_access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", aws_access_key)\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", aws_secret_key)\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "# Read from S3\n",
    "df = spark.read.parquet(\"s3a://mailshake-analytics/raw/client_id=client_1/entity=campaign/date=2025-12-25/campaign.parquet\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b322be6a-8d87-4bf0-ae76-ba20083f6e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- object: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- created: string (nullable = true)\n",
      " |-- isArchived: boolean (nullable = true)\n",
      " |-- isPaused: boolean (nullable = true)\n",
      " |-- wizardStatus: string (nullable = true)\n",
      " |-- messages: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- isPaused: boolean (nullable = true)\n",
      " |    |    |-- object: string (nullable = true)\n",
      " |    |    |-- replyToID: long (nullable = true)\n",
      " |    |    |-- subject: string (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- sender.object: string (nullable = true)\n",
      " |-- sender.id: string (nullable = true)\n",
      " |-- sender.emailAddress: string (nullable = true)\n",
      " |-- sender.fromName: string (nullable = true)\n",
      " |-- sender.created: string (nullable = true)\n",
      " |-- processed_timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"s3a://mailshake-analytics/raw/client_id=client_1/entity=campaign/date=2025-12-25/campaign_modified.parquet\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb3865e-1ac3-49a4-8782-218515e2e3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ============================================================================\n",
    "# SPARK SESSION\n",
    "# ============================================================================\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CampaignMessagesCurationIncremental\") \\\n",
    "    .config(\n",
    "        \"spark.driver.extraClassPath\",\n",
    "        \"/opt/spark/jars/hadoop-aws-3.3.4.jar:/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "    ) \\\n",
    "    .config(\n",
    "        \"spark.executor.extraClassPath\",\n",
    "        \"/opt/spark/jars/hadoop-aws-3.3.4.jar:/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "    ) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# ============================================================================\n",
    "# S3 CONFIG\n",
    "# ============================================================================\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", os.getenv(\"AWS_ACCESS_KEY_ID\"))\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", os.getenv(\"AWS_SECRET_ACCESS_KEY\"))\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIG\n",
    "# ============================================================================\n",
    "LOAD_TYPE = os.getenv(\"LOAD_TYPE\", \"incremental\")  # incremental | full\n",
    "\n",
    "BASE_INPUT_PATH = \"s3a://mailshake-analytics/raw\"\n",
    "BASE_OUTPUT_PATH = \"s3a://mailshake-analytics/curated\"\n",
    "CURATED_PATH = f\"{BASE_OUTPUT_PATH}/entity=campaign_messages/campaign_messages_curated.parquet\"\n",
    "\n",
    "CLIENT_IDS = [\"client_1\", \"client_2\", \"client_3\"]\n",
    "\n",
    "# ============================================================================\n",
    "# DETERMINE DATES TO PROCESS (TRUE INCREMENTAL)\n",
    "# ============================================================================\n",
    "dates_to_process = []\n",
    "\n",
    "if LOAD_TYPE == \"incremental\":\n",
    "    try:\n",
    "        existing_df = spark.read.parquet(CURATED_PATH)\n",
    "        last_processed_date = (\n",
    "            existing_df\n",
    "            .select(max(\"source_date\").alias(\"max_date\"))\n",
    "            .collect()[0][\"max_date\"]\n",
    "        )\n",
    "\n",
    "        print(f\"üìå Last processed date: {last_processed_date}\")\n",
    "        start_date = datetime.strptime(str(last_processed_date), \"%Y-%m-%d\") + timedelta(days=1)\n",
    "\n",
    "    except Exception:\n",
    "        print(\"‚ö†Ô∏è No curated data found. Bootstrapping full load.\")\n",
    "        start_date = datetime.strptime(\"2025-12-20\", \"%Y-%m-%d\")\n",
    "\n",
    "    end_date = datetime.today()\n",
    "\n",
    "    dates_to_process = [\n",
    "        (start_date + timedelta(days=i)).strftime(\"%Y-%m-%d\")\n",
    "        for i in range((end_date - start_date).days + 1)\n",
    "    ]\n",
    "\n",
    "else:\n",
    "    START_DATE = os.getenv(\"START_DATE\", \"2025-12-20\")\n",
    "    END_DATE = os.getenv(\"END_DATE\", datetime.today().strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "    start_date = datetime.strptime(START_DATE, \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(END_DATE, \"%Y-%m-%d\")\n",
    "\n",
    "    dates_to_process = [\n",
    "        (start_date + timedelta(days=i)).strftime(\"%Y-%m-%d\")\n",
    "        for i in range((end_date - start_date).days + 1)\n",
    "    ]\n",
    "\n",
    "if not dates_to_process:\n",
    "    print(\"‚úÖ No new dates to process. Exiting.\")\n",
    "    spark.stop()\n",
    "    exit(0)\n",
    "\n",
    "print(f\"üìÖ Dates to process: {dates_to_process}\")\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER: FLATTEN STRUCT COLUMNS\n",
    "# ============================================================================\n",
    "def flatten_struct_columns(df):\n",
    "    flat_cols = []\n",
    "    for field in df.schema.fields:\n",
    "        if isinstance(field.dataType, StructType):\n",
    "            for sub in field.dataType.fields:\n",
    "                flat_cols.append(\n",
    "                    col(f\"{field.name}.{sub.name}\").alias(f\"{field.name}_{sub.name}\")\n",
    "                )\n",
    "        else:\n",
    "            flat_cols.append(col(field.name))\n",
    "    return df.select(flat_cols)\n",
    "\n",
    "# ============================================================================\n",
    "# READ RAW DATA\n",
    "# ============================================================================\n",
    "all_dfs = []\n",
    "\n",
    "for process_date in dates_to_process:\n",
    "    for client_id in CLIENT_IDS:\n",
    "        try:\n",
    "            input_path = (\n",
    "                f\"{BASE_INPUT_PATH}/client_id={client_id}/entity=campaign/\"\n",
    "                f\"date={process_date}/campaign.parquet\"\n",
    "            )\n",
    "\n",
    "            print(f\"üìÇ Reading {client_id} | {process_date}\")\n",
    "            df = spark.read.parquet(input_path)\n",
    "\n",
    "            df = (\n",
    "                df.withColumn(\"client_id\", lit(client_id))\n",
    "                  .withColumn(\"source_date\", lit(process_date))\n",
    "            )\n",
    "\n",
    "            all_dfs.append(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skipping {client_id} {process_date}: {e}\")\n",
    "\n",
    "if not all_dfs:\n",
    "    print(\"‚ùå No data loaded.\")\n",
    "    spark.stop()\n",
    "    exit(1)\n",
    "\n",
    "combined_df = all_dfs[0]\n",
    "for df in all_dfs[1:]:\n",
    "    combined_df = combined_df.unionByName(df, allowMissingColumns=True)\n",
    "\n",
    "# ============================================================================\n",
    "# EXPLODE MESSAGES\n",
    "# ============================================================================\n",
    "exploded_df = combined_df.withColumn(\n",
    "    \"message\",\n",
    "    explode_outer(col(\"messages\"))\n",
    ").drop(\"messages\")\n",
    "\n",
    "# ============================================================================\n",
    "# FLATTEN STRUCTS\n",
    "# ============================================================================\n",
    "flattened_df = flatten_struct_columns(exploded_df)\n",
    "\n",
    "# ============================================================================\n",
    "# METADATA\n",
    "# ============================================================================\n",
    "curated_df = (\n",
    "    flattened_df\n",
    "    .withColumn(\"processing_timestamp\", current_timestamp())\n",
    "    .withColumn(\"load_type\", lit(LOAD_TYPE))\n",
    "    .withColumn(\"processing_date\", current_date())\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# DEDUPLICATION (CRITICAL)\n",
    "# ============================================================================\n",
    "curated_df = curated_df.dropDuplicates(\n",
    "    [\"client_id\", \"id\", \"message_id\", \"source_date\"]\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# WRITE (DYNAMIC PARTITION OVERWRITE)\n",
    "# ============================================================================\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "\n",
    "curated_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"client_id\", \"source_date\") \\\n",
    "    .parquet(CURATED_PATH)\n",
    "\n",
    "print(\"‚úÖ Write complete\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "print(\"üìä Records written:\", curated_df.count())\n",
    "curated_df.groupBy(\"client_id\", \"source_date\").count().show()\n",
    "\n",
    "spark.stop()\n",
    "print(\"üéâ Job completed successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d682c1a-0ef7-47b6-8762-c608591d1643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/30 15:05:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Single-date mode enabled: 2025-12-25\n",
      "üìÖ Dates to process: ['2025-12-25']\n",
      "üìÇ Reading client_1 | 2025-12-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/30 15:05:39 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Reading client_2 | 2025-12-25\n",
      "üìÇ Reading client_3 | 2025-12-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/30 15:05:52 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Write complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Records written: 75\n",
      "+---------+-----------+-----+\n",
      "|client_id|source_date|count|\n",
      "+---------+-----------+-----+\n",
      "| client_1| 2025-12-25|   40|\n",
      "| client_3| 2025-12-25|   12|\n",
      "| client_2| 2025-12-25|   23|\n",
      "+---------+-----------+-----+\n",
      "\n",
      "üéâ Job completed successfully\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# SPARK SESSION\n",
    "# ============================================================================\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CampaignMessagesCuration\") \\\n",
    "    .config(\n",
    "        \"spark.driver.extraClassPath\",\n",
    "        \"/opt/spark/jars/hadoop-aws-3.3.4.jar:/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "    ) \\\n",
    "    .config(\n",
    "        \"spark.executor.extraClassPath\",\n",
    "        \"/opt/spark/jars/hadoop-aws-3.3.4.jar:/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "    ) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# ============================================================================\n",
    "# S3 CONFIG\n",
    "# ============================================================================\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", os.getenv(\"AWS_ACCESS_KEY_ID\"))\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", os.getenv(\"AWS_SECRET_ACCESS_KEY\"))\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIG\n",
    "# ============================================================================\n",
    "BASE_INPUT_PATH = \"s3a://mailshake-analytics/raw\"\n",
    "BASE_OUTPUT_PATH = \"s3a://mailshake-analytics/curated\"\n",
    "OUTPUT_PATH = f\"{BASE_OUTPUT_PATH}/entity=campaign/\"\n",
    "\n",
    "CLIENT_IDS = [\"client_1\", \"client_2\", \"client_3\"]\n",
    "\n",
    "# Single-date override for backfill / fix\n",
    "PROCESS_DATE = \"2025-12-25\"  # change as needed; set None for incremental\n",
    "\n",
    "# ============================================================================\n",
    "# DETERMINE DATES TO PROCESS\n",
    "# ============================================================================\n",
    "dates_to_process = []\n",
    "\n",
    "if PROCESS_DATE:\n",
    "    print(f\"üìÖ Single-date mode enabled: {PROCESS_DATE}\")\n",
    "    dates_to_process = [PROCESS_DATE]\n",
    "else:\n",
    "    print(\"üîÑ Incremental mode enabled\")\n",
    "    try:\n",
    "        existing_df = spark.read.parquet(OUTPUT_PATH)\n",
    "        last_processed_date = existing_df.select(max(\"source_date\").alias(\"max_date\")).collect()[0][\"max_date\"]\n",
    "        print(f\"üìå Last processed date: {last_processed_date}\")\n",
    "        start_date = datetime.strptime(str(last_processed_date), \"%Y-%m-%d\") + timedelta(days=1)\n",
    "    except Exception:\n",
    "        print(\"‚ö†Ô∏è No curated data found. Bootstrapping from initial date.\")\n",
    "        start_date = datetime.strptime(\"2025-12-20\", \"%Y-%m-%d\")\n",
    "    \n",
    "    end_date = datetime.today()\n",
    "    dates_to_process = [(start_date + timedelta(days=i)).strftime(\"%Y-%m-%d\")\n",
    "                        for i in range((end_date - start_date).days + 1)]\n",
    "\n",
    "if not dates_to_process:\n",
    "    print(\"‚úÖ No dates to process. Exiting cleanly.\")\n",
    "    spark.stop()\n",
    "    exit(0)\n",
    "\n",
    "print(f\"üìÖ Dates to process: {dates_to_process}\")\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER: FLATTEN STRUCT COLUMNS (SAFE WITH BACKTICKS)\n",
    "# ============================================================================\n",
    "def flatten_struct_columns(df):\n",
    "    flat_cols = []\n",
    "    for field in df.schema.fields:\n",
    "        name = field.name\n",
    "        if isinstance(field.dataType, StructType):\n",
    "            for sub in field.dataType.fields:\n",
    "                # Keep original name but access with backticks for dots\n",
    "                full_name = f\"`{name}`.`{sub.name}`\"\n",
    "                alias_name = f\"{name}_{sub.name}\"\n",
    "                flat_cols.append(col(full_name).alias(alias_name))\n",
    "        else:\n",
    "            flat_cols.append(col(f\"`{name}`\"))\n",
    "    return df.select(flat_cols)\n",
    "\n",
    "# ============================================================================\n",
    "# READ RAW DATA\n",
    "# ============================================================================\n",
    "dfs = []\n",
    "\n",
    "for process_date in dates_to_process:\n",
    "    for client_id in CLIENT_IDS:\n",
    "        try:\n",
    "            input_path = f\"{BASE_INPUT_PATH}/client_id={client_id}/entity=campaign/date={process_date}/campaign.parquet\"\n",
    "            print(f\"üìÇ Reading {client_id} | {process_date}\")\n",
    "            df = spark.read.parquet(input_path)\n",
    "            df = df.withColumn(\"client_id\", lit(client_id)) \\\n",
    "                   .withColumn(\"source_date\", lit(process_date))\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skipping {client_id} {process_date}: {e}\")\n",
    "\n",
    "if not dfs:\n",
    "    print(\"‚ùå No raw data loaded.\")\n",
    "    spark.stop()\n",
    "    exit(1)\n",
    "\n",
    "combined_df = dfs[0]\n",
    "for df in dfs[1:]:\n",
    "    combined_df = combined_df.unionByName(df, allowMissingColumns=True)\n",
    "\n",
    "# ============================================================================\n",
    "# EXPLODE MESSAGES\n",
    "# ============================================================================\n",
    "exploded_df = combined_df.withColumn(\"message\", explode_outer(col(\"messages\"))).drop(\"messages\")\n",
    "\n",
    "# ============================================================================\n",
    "# FLATTEN STRUCTS\n",
    "# ============================================================================\n",
    "flattened_df = flatten_struct_columns(exploded_df)\n",
    "\n",
    "# ============================================================================\n",
    "# ADD METADATA\n",
    "# ============================================================================\n",
    "curated_df = flattened_df.withColumn(\"processing_timestamp\", current_timestamp()) \\\n",
    "    .withColumn(\"load_type\", lit(\"single_date\") if PROCESS_DATE else lit(\"incremental\")) \\\n",
    "    .withColumn(\"processing_date\", current_date())\n",
    "\n",
    "# ============================================================================\n",
    "# DEDUPLICATE (RERUN SAFE)\n",
    "# ============================================================================\n",
    "curated_df = curated_df.dropDuplicates([\"client_id\", \"id\", \"message_id\", \"source_date\"])\n",
    "\n",
    "# ============================================================================\n",
    "# WRITE (DYNAMIC PARTITION OVERWRITE)\n",
    "# ============================================================================\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "curated_df.write.mode(\"overwrite\").partitionBy(\"client_id\", \"source_date\").parquet(OUTPUT_PATH)\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "print(\"‚úÖ Write complete\")\n",
    "print(\"üìä Records written:\", curated_df.count())\n",
    "curated_df.groupBy(\"client_id\", \"source_date\").count().show()\n",
    "\n",
    "spark.stop()\n",
    "print(\"üéâ Job completed successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c940a843-6d50-4535-8f12-314d28aac68f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20cc9d7b-4a30-4e44-8bcc-35833e007154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# SPARK SESSION\n",
    "# ============================================================================\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CampaignMessagesCuration\") \\\n",
    "    .config(\n",
    "        \"spark.driver.extraClassPath\",\n",
    "        \"/opt/spark/jars/hadoop-aws-3.3.4.jar:/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "    ) \\\n",
    "    .config(\n",
    "        \"spark.executor.extraClassPath\",\n",
    "        \"/opt/spark/jars/hadoop-aws-3.3.4.jar:/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "    ) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# ============================================================================\n",
    "# S3 CONFIG\n",
    "# ============================================================================\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", os.getenv(\"AWS_ACCESS_KEY_ID\"))\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", os.getenv(\"AWS_SECRET_ACCESS_KEY\"))\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIG\n",
    "# ============================================================================\n",
    "BASE_INPUT_PATH = \"s3a://mailshake-analytics/raw\"\n",
    "BASE_OUTPUT_PATH = \"s3a://mailshake-analytics/curated\"\n",
    "\n",
    "df_activity_open = spark.read.parquet(\"s3a://mailshake-analytics/raw/client_id=client_1/entity=activity_open/date=2025-12-25/activity_open_20251226T002548.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "830cba88-985d-4b5a-8509-3bb14a7ef990",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_activity_reply = spark.read.parquet(\"s3a://mailshake-analytics/raw/client_id=client_1/entity=activity_reply/date=2025-12-25/activity_reply_20251226T002549.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa2a8a3b-0137-4958-8d5d-c3dd4d9d783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_activity_sent = spark.read.parquet(\"s3a://mailshake-analytics/raw/client_id=client_1/entity=activity_sent/date=2025-12-25/activity_sent_20251226T002549.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b994fd3f-266f-4286-be30-3f4c7f2f2982",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_created_leads = spark.read.parquet(\"s3a://mailshake-analytics/raw/client_id=client_1/entity=created_leads/date=2025-12-25/created_leads_20251226T002549.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bfd4fe2-74ce-4ab4-81e1-7b01082be9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- object: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- actionDate: string (nullable = true)\n",
      " |-- isDuplicate: boolean (nullable = true)\n",
      " |-- recipient.object: string (nullable = true)\n",
      " |-- recipient.id: long (nullable = true)\n",
      " |-- recipient.emailAddress: string (nullable = true)\n",
      " |-- recipient.fullName: string (nullable = true)\n",
      " |-- recipient.created: string (nullable = true)\n",
      " |-- recipient.isPaused: boolean (nullable = true)\n",
      " |-- recipient.contactID: long (nullable = true)\n",
      " |-- recipient.first: string (nullable = true)\n",
      " |-- recipient.last: string (nullable = true)\n",
      " |-- recipient.fields.link: string (nullable = true)\n",
      " |-- recipient.fields.position: string (nullable = true)\n",
      " |-- recipient.fields.date applied: string (nullable = true)\n",
      " |-- recipient.fields.account: string (nullable = true)\n",
      " |-- recipient.fields.phoneNumber: string (nullable = true)\n",
      " |-- recipient.fields.facebookUrl: string (nullable = true)\n",
      " |-- recipient.fields.instagramID: string (nullable = true)\n",
      " |-- recipient.fields.linkedInUrl: string (nullable = true)\n",
      " |-- recipient.fields.twitterID: string (nullable = true)\n",
      " |-- campaign.object: string (nullable = true)\n",
      " |-- campaign.id: long (nullable = true)\n",
      " |-- campaign.title: string (nullable = true)\n",
      " |-- campaign.wizardStatus: integer (nullable = true)\n",
      " |-- parent.object: string (nullable = true)\n",
      " |-- parent.id: long (nullable = true)\n",
      " |-- parent.type: string (nullable = true)\n",
      " |-- parent.message.object: string (nullable = true)\n",
      " |-- parent.message.id: long (nullable = true)\n",
      " |-- parent.message.type: string (nullable = true)\n",
      " |-- parent.message.subject: string (nullable = true)\n",
      " |-- parent.message.replyToID: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_activity_open.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8a2786f-e973-4764-b906-579038b02f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- object: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- actionDate: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- subject: string (nullable = true)\n",
      " |-- externalID: string (nullable = true)\n",
      " |-- externalRawMessageID: string (nullable = true)\n",
      " |-- externalConversationID: string (nullable = true)\n",
      " |-- rawBody: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- plainTextBody: string (nullable = true)\n",
      " |-- recipient.object: string (nullable = true)\n",
      " |-- recipient.id: long (nullable = true)\n",
      " |-- recipient.emailAddress: string (nullable = true)\n",
      " |-- recipient.fullName: string (nullable = true)\n",
      " |-- recipient.created: string (nullable = true)\n",
      " |-- recipient.isPaused: boolean (nullable = true)\n",
      " |-- recipient.contactID: long (nullable = true)\n",
      " |-- recipient.first: string (nullable = true)\n",
      " |-- recipient.last: string (nullable = true)\n",
      " |-- recipient.fields.account: string (nullable = true)\n",
      " |-- recipient.fields.phoneNumber: string (nullable = true)\n",
      " |-- recipient.fields.facebookUrl: string (nullable = true)\n",
      " |-- recipient.fields.instagramID: string (nullable = true)\n",
      " |-- recipient.fields.linkedInUrl: string (nullable = true)\n",
      " |-- recipient.fields.twitterID: string (nullable = true)\n",
      " |-- campaign.object: string (nullable = true)\n",
      " |-- campaign.id: long (nullable = true)\n",
      " |-- campaign.title: string (nullable = true)\n",
      " |-- campaign.wizardStatus: integer (nullable = true)\n",
      " |-- parent.object: string (nullable = true)\n",
      " |-- parent.id: long (nullable = true)\n",
      " |-- parent.type: string (nullable = true)\n",
      " |-- parent.message.object: string (nullable = true)\n",
      " |-- parent.message.id: long (nullable = true)\n",
      " |-- parent.message.type: string (nullable = true)\n",
      " |-- parent.message.subject: string (nullable = true)\n",
      " |-- parent.message.replyToID: double (nullable = true)\n",
      " |-- from.object: string (nullable = true)\n",
      " |-- from.address: string (nullable = true)\n",
      " |-- from.fullName: integer (nullable = true)\n",
      " |-- from.first: string (nullable = true)\n",
      " |-- from.last: string (nullable = true)\n",
      " |-- recipient.fields.link: string (nullable = true)\n",
      " |-- recipient.fields.position: string (nullable = true)\n",
      " |-- recipient.fields.date applied: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_activity_reply.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a50fe8-b0ba-45dd-b311-6cbcdc4420c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_activity_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcf58f08-60dd-41ab-9de2-067204e317fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- object: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- actionDate: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- excludeBody: boolean (nullable = true)\n",
      " |-- to: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- address: string (nullable = true)\n",
      " |    |    |-- first: string (nullable = true)\n",
      " |    |    |-- fullName: integer (nullable = true)\n",
      " |    |    |-- last: string (nullable = true)\n",
      " |    |    |-- object: string (nullable = true)\n",
      " |-- subject: string (nullable = true)\n",
      " |-- externalID: string (nullable = true)\n",
      " |-- externalRawMessageID: string (nullable = true)\n",
      " |-- externalConversationID: string (nullable = true)\n",
      " |-- rawBody: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- plainTextBody: string (nullable = true)\n",
      " |-- recipient.object: string (nullable = true)\n",
      " |-- recipient.id: long (nullable = true)\n",
      " |-- recipient.emailAddress: string (nullable = true)\n",
      " |-- recipient.fullName: string (nullable = true)\n",
      " |-- recipient.created: string (nullable = true)\n",
      " |-- recipient.isPaused: boolean (nullable = true)\n",
      " |-- recipient.first: string (nullable = true)\n",
      " |-- recipient.last: string (nullable = true)\n",
      " |-- recipient.fields.account: string (nullable = true)\n",
      " |-- recipient.fields.phoneNumber: string (nullable = true)\n",
      " |-- recipient.fields.facebookUrl: string (nullable = true)\n",
      " |-- recipient.fields.instagramID: string (nullable = true)\n",
      " |-- recipient.fields.linkedInUrl: string (nullable = true)\n",
      " |-- recipient.fields.twitterID: string (nullable = true)\n",
      " |-- campaign.object: string (nullable = true)\n",
      " |-- campaign.id: long (nullable = true)\n",
      " |-- campaign.title: string (nullable = true)\n",
      " |-- campaign.wizardStatus: integer (nullable = true)\n",
      " |-- message.object: string (nullable = true)\n",
      " |-- message.id: long (nullable = true)\n",
      " |-- message.type: string (nullable = true)\n",
      " |-- message.subject: string (nullable = true)\n",
      " |-- message.replyToID: double (nullable = true)\n",
      " |-- from.object: string (nullable = true)\n",
      " |-- from.address: string (nullable = true)\n",
      " |-- from.fullName: string (nullable = true)\n",
      " |-- from.first: string (nullable = true)\n",
      " |-- from.last: string (nullable = true)\n",
      " |-- recipient.fields.link: string (nullable = true)\n",
      " |-- recipient.fields.position: string (nullable = true)\n",
      " |-- recipient.fields.date applied: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_activity_sent.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5488071d-7c38-4f9f-b12f-0d19aae7dd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------------------+--------------------+--------------------+----------+------+----------------+------------+----------------------+------------------+--------------------+------------------+-------------------+---------------+--------------+---------------------+-------------------------+-----------------------------+------------------------+----------------------------+----------------------------+----------------------------+----------------------------+--------------------------+---------------+-----------+--------------------+---------------------+\n",
      "|object|      id|             created|          openedDate|lastStatusChangeDate|annotation|status|recipient.object|recipient.id|recipient.emailAddress|recipient.fullName|   recipient.created|recipient.isPaused|recipient.contactID|recipient.first|recipient.last|recipient.fields.link|recipient.fields.position|recipient.fields.date applied|recipient.fields.account|recipient.fields.phoneNumber|recipient.fields.facebookUrl|recipient.fields.instagramID|recipient.fields.linkedInUrl|recipient.fields.twitterID|campaign.object|campaign.id|      campaign.title|campaign.wizardStatus|\n",
      "+------+--------+--------------------+--------------------+--------------------+----------+------+----------------+------------+----------------------+------------------+--------------------+------------------+-------------------+---------------+--------------+---------------------+-------------------------+-----------------------------+------------------------+----------------------------+----------------------------+----------------------------+----------------------------+--------------------------+---------------+-----------+--------------------+---------------------+\n",
      "|  lead|15694961|2025-12-24T14:29:...|2025-12-24T14:29:...|                NULL|      NULL|  open|       recipient|   670904851|  bhs@peoplebrandco...|                  |2025-12-19T14:13:...|             false|          398378572|               |              | https://wd3.mywor...|             Data Analyst|                     12/16/25|                   Magna|                            |                            |                            |                            |                          |       campaign|    1488093|2025_12_Vansh_Gup...|                 NULL|\n",
      "|  lead|15693354|2025-12-22T17:21:...|2025-12-22T17:21:...|                NULL|      NULL|  open|       recipient|   670904869|  josh@inspirationm...|                  |2025-12-19T14:13:...|             false|          398378590|               |              | https://ats.rippl...|     Analyst, Business...|                   18-12-2025|    Inspiration Mobility|                            |                            |                            |                            |                          |       campaign|    1488093|2025_12_Vansh_Gup...|                 NULL|\n",
      "|  lead|15691363|2025-12-19T15:59:...|2025-12-19T15:59:...|                NULL|      NULL|  open|       recipient|   657547516|     chelsea@hebbia.ai|                  |2025-09-22T02:21:...|             false|          390938803|               |              | https://job-board...|            Data Engineer|             22-09-2025 00:00|                  Hebbia|                            |                            |                            |                            |                          |       campaign|    1425327|2025_06_Megha_hol...|                 NULL|\n",
      "|  lead|15690966|2025-12-19T11:56:...|2025-12-19T11:56:...|                NULL|      NULL|  open|       recipient|   657462099|  sara.banaszak@bet...|                  |2025-09-19T15:11:...|             false|          390877419|               |              | https://jobs.ashb...|     Senior Data Platf...|             19-09-2025 00:00|                BetterUp|                            |                            |                            |                            |                          |       campaign|    1425327|2025_06_Megha_hol...|                 NULL|\n",
      "|  lead|15689069|2025-12-17T21:30:...|2025-12-17T21:30:...|                NULL|      NULL|  open|       recipient|   657462091|  leslie@shepherdin...|                  |2025-09-19T15:11:...|             false|          390877412|               |              | https://jobs.ashb...|            Data Engineer|             18-09-2025 00:00|                Shepherd|                            |                            |                            |                            |                          |       campaign|    1425327|2025_06_Megha_hol...|                 NULL|\n",
      "|  lead|15687765|2025-12-17T04:33:...|2025-12-17T04:33:...|                NULL|      NULL|  open|       recipient|   656709044|     hassan@zyphra.com|                  |2025-09-16T02:18:...|             false|          390464662|               |              | https://jobs.ashb...|     Data Engineer - M...|             15-09-2025 00:00|                  Zyphra|                            |                            |                            |                            |                          |       campaign|    1425327|2025_06_Megha_hol...|                 NULL|\n",
      "|  lead|15681423|2025-12-11T17:27:...|2025-12-11T17:27:...|                NULL|      NULL|  open|       recipient|   650070862|  ella@globalchanne...|                  |2025-08-06T10:15:...|             false|          386667719|               |              |                 NULL|                     NULL|                         NULL|    Global Channel Ma...|                            |                            |                            |                            |                          |       campaign|    1447546|Cold_Emailing_Meg...|                 NULL|\n",
      "|  lead|15677415|2025-12-09T16:21:...|2025-12-09T16:21:...|                NULL|      NULL|  open|       recipient|   653207637|  dwanna.straetker@...|                  |2025-08-25T08:40:...|             false|          388502196|               |              | https://jobs.riot...|     Senior Data Engin...|                   22-08-2025|               Rio Tinto|                            |                            |                            |                            |                          |       campaign|    1425327|2025_06_Megha_hol...|                 NULL|\n",
      "|  lead|15675494|2025-12-08T15:51:...|2025-12-08T15:51:...|                NULL|      NULL|  open|       recipient|   652103035|  apahilajani@ontra.ai|                  |2025-08-18T14:00:...|             false|          387809861|               |              | https://www.ontra...|     Senior Data Engineer|                   18-08-2025|                Ontra.ai|                            |                            |                            |                            |                          |       campaign|    1425327|2025_06_Megha_hol...|                 NULL|\n",
      "|  lead|15669725|2025-12-03T16:51:...|2025-12-03T16:51:...|                NULL|      NULL|  open|       recipient|   650070597|  cyrina.avila@omad...|                  |2025-08-06T10:15:...|             false|          386667458|               |              |                 NULL|                     NULL|                         NULL|            Omada Health|                            |                            |                            |                            |                          |       campaign|    1447546|Cold_Emailing_Meg...|                 NULL|\n",
      "|  lead|15664895|2025-12-01T13:16:...|2025-12-01T13:16:...|                NULL|      NULL|  open|       recipient|   650068864|     mcho@rapidsos.com|                  |2025-08-06T10:15:...|             false|          386666188|               |              |                 NULL|                     NULL|                         NULL|                RapidSOS|                            |                            |                            |                            |                          |       campaign|    1447546|Cold_Emailing_Meg...|                 NULL|\n",
      "|  lead|15660061|2025-11-26T16:29:...|2025-11-26T16:29:...|                NULL|      NULL|  open|       recipient|   650070527|  greg.tuttle@wowin...|                  |2025-08-06T10:15:...|             false|          386667388|               |              |                 NULL|                     NULL|                         NULL|                     Wow|                            |                            |                            |                            |                          |       campaign|    1447546|Cold_Emailing_Meg...|                 NULL|\n",
      "|  lead|15657905|2025-11-25T13:57:...|2025-11-25T13:57:...|                NULL|      NULL|  open|       recipient|   650070554|  jpritchard29@bloo...|                  |2025-08-06T10:15:...|             false|          386667415|               |              |                 NULL|                     NULL|                         NULL|               Bloomberg|                            |                            |                            |                            |                          |       campaign|    1447546|Cold_Emailing_Meg...|                 NULL|\n",
      "|  lead|15656516|2025-11-24T18:01:...|2025-11-24T18:01:...|                NULL|      NULL|  open|       recipient|   650068840|  s.taryla@fetchrew...|                  |2025-08-06T10:15:...|             false|          381198189|               |              | https://job-board...|            Data Engineer|                   17-06-2025|                   Fetch|                            |                            |                            |                            |                          |       campaign|    1447546|Cold_Emailing_Meg...|                 NULL|\n",
      "|  lead|15653008|2025-11-20T22:13:...|2025-11-20T22:13:...|                NULL|      NULL|  open|       recipient|   650070519|        tasteam@nu.edu|                  |2025-08-06T10:15:...|             false|          386667380|               |              |                 NULL|                     NULL|                         NULL|     National University|                            |                            |                            |                            |                          |       campaign|    1447546|Cold_Emailing_Meg...|                 NULL|\n",
      "|  lead|15642494|2025-11-12T15:15:...|2025-11-12T15:15:...|                NULL|      NULL|  open|       recipient|   665379146|  johnguercio@loopr...|                  |2025-11-09T18:30:...|             false|          386665959|               |              | https://jobs.leve...|            Data Engineer|                   06-11-2025|                    Loop|                            |                            |                            |                            |                          |       campaign|    1458402|2025_09_Avinash_P...|                 NULL|\n",
      "|  lead|15636650|2025-11-07T03:47:...|2025-11-07T03:47:...|                NULL|      NULL|  open|       recipient|   665084583|       abhi@mercor.com|                  |2025-11-06T05:10:...|             false|          394963405|               |              | https://jobs.ashb...|            Data Engineer|             05-11-2025 00:00|                  Mercor|                            |                            |                            |                            |                          |       campaign|    1425327|2025_06_Megha_hol...|                 NULL|\n",
      "|  lead|15635940|2025-11-06T16:12:...|2025-11-06T16:12:...|                NULL|      NULL|  open|       recipient|   663992585|  gianna.canning@nu...|                  |2025-10-29T18:34:...|             false|          394490002|               |              | https://recruit.h...|            Data Engineer|                   29-10-2025|               Numerator|                            |                            |                            |                            |                          |       campaign|    1458402|2025_09_Avinash_P...|                 NULL|\n",
      "|  lead|15634369|2025-11-05T19:15:...|2025-11-05T19:15:...|                NULL|      NULL|  open|       recipient|   664216027|         n@kiwibot.com|                  |2025-10-31T09:55:...|             false|          394599444|               |              | https://kiwibot.n...|            Data Engineer|                   30-10-2025|                 Kiwibot|                            |                            |                            |                            |                          |       campaign|    1458402|2025_09_Avinash_P...|                 NULL|\n",
      "|  lead|15634144|2025-11-05T17:27:...|2025-11-05T17:27:...|                NULL|      NULL|  open|       recipient|   663508747|  lyndsey@hellotilt...|                  |2025-10-25T10:47:...|             false|          394192223|               |              | https://hellotilt...|     Data Engineer @ Tilt|             24-10-2025 00:00|                    Tilt|                            |                            |                            |                            |                          |       campaign|    1425327|2025_06_Megha_hol...|                 NULL|\n",
      "+------+--------+--------------------+--------------------+--------------------+----------+------+----------------+------------+----------------------+------------------+--------------------+------------------+-------------------+---------------+--------------+---------------------+-------------------------+-----------------------------+------------------------+----------------------------+----------------------------+----------------------------+----------------------------+--------------------------+---------------+-----------+--------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_created_leads.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba9f3ade-65d3-420a-9fcb-61be39e2d415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single date: 2025-12-25\n",
      "['client_1', 'client_2', 'client_3']\n",
      "üìÖ Single-date mode enabled: 2025-12-25\n",
      "{'client_1': ['2025-12-25'], 'client_2': ['2025-12-25'], 'client_3': ['2025-12-25']}\n",
      "üìÇ activity_open | client_1 | 2025-12-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 100 records\n",
      "üìÇ activity_open | client_2 | 2025-12-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 100 records\n",
      "üìÇ activity_open | client_3 | 2025-12-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 100 records\n",
      "üéØ activity_open total records written: 300\n",
      "üìÇ activity_reply | client_1 | 2025-12-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 25 records\n",
      "üìÇ activity_reply | client_2 | 2025-12-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 25 records\n",
      "üìÇ activity_reply | client_3 | 2025-12-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 25 records\n",
      "üéØ activity_reply total records written: 75\n",
      "üìÇ activity_sent | client_1 | 2025-12-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 25 records\n",
      "üìÇ activity_sent | client_2 | 2025-12-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 25 records\n",
      "üìÇ activity_sent | client_3 | 2025-12-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 25 records\n",
      "üéØ activity_sent total records written: 75\n",
      "üìÇ created_leads | client_1 | 2025-12-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 100 records\n",
      "üìÇ created_leads | client_2 | 2025-12-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 43 records\n",
      "üìÇ created_leads | client_3 | 2025-12-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 16 records\n",
      "üéØ created_leads total records written: 159\n",
      "üéâ All datasets processed successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, max, lit, current_timestamp, current_date, explode_outer\n",
    "from pyspark.sql.types import StructType\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# SPARK SESSION\n",
    "# ============================================================================\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MailshakeCampaignCurations\") \\\n",
    "    .config(\n",
    "        \"spark.driver.extraClassPath\",\n",
    "        \"/opt/spark/jars/hadoop-aws-3.3.4.jar:/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "    ) \\\n",
    "    .config(\n",
    "        \"spark.executor.extraClassPath\",\n",
    "        \"/opt/spark/jars/hadoop-aws-3.3.4.jar:/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "    ) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# ============================================================================\n",
    "# S3 CONFIG\n",
    "# ============================================================================\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", os.getenv(\"AWS_ACCESS_KEY_ID\"))\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", os.getenv(\"AWS_SECRET_ACCESS_KEY\"))\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIG\n",
    "# ============================================================================\n",
    "RAW_PATH = \"s3a://mailshake-analytics/raw\"\n",
    "CURATED_PATH = \"s3a://mailshake-analytics/curated\"\n",
    "CLIENT_IDS = [\"client_1\", \"client_2\", \"client_3\"]\n",
    "\n",
    "# SINGLE DATE MODE: set this to a date string to process just 1 date\n",
    "# For incremental, leave as None\n",
    "SINGLE_DATE = \"2025-12-25\"\n",
    "\n",
    "# Bootstrap date if no data exists\n",
    "BOOTSTRAP_DATE = \"2025-12-20\"\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER: Flatten struct columns\n",
    "# ============================================================================\n",
    "def flatten_struct_columns(df):\n",
    "    flat_cols = []\n",
    "    for field in df.schema.fields:\n",
    "        name = field.name\n",
    "        if isinstance(field.dataType, StructType):\n",
    "            for sub in field.dataType.fields:\n",
    "                full_name = f\"`{name}`.`{sub.name}`\"\n",
    "                alias_name = f\"{name}_{sub.name}\"\n",
    "                flat_cols.append(col(full_name).alias(alias_name))\n",
    "        else:\n",
    "            flat_cols.append(col(f\"`{name}`\"))\n",
    "    return df.select(flat_cols)\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER: Get dates to process per client\n",
    "# ============================================================================\n",
    "def get_dates_to_process(curated_path, client_ids, single_date=None, bootstrap_date=\"2025-12-20\"):\n",
    "    print(\"Single date:\", single_date)\n",
    "    print(client_ids)\n",
    "    if single_date:\n",
    "        print(f\"üìÖ Single-date mode enabled: {single_date}\")\n",
    "        return {client: [single_date] for client in client_ids}\n",
    "\n",
    "    # Incremental mode\n",
    "    dates_to_process = {}\n",
    "    try:\n",
    "        existing_df = spark.read.parquet(curated_path)\n",
    "        last_dates = existing_df.groupBy(\"client_id\").agg(max(\"source_date\").alias(\"last_date\")).collect()\n",
    "        last_date_dict = {row[\"client_id\"]: row[\"last_date\"] for row in last_dates}\n",
    "    except Exception:\n",
    "        print(\"‚ö†Ô∏è No curated data found. Bootstrapping all clients.\")\n",
    "        last_date_dict = {}\n",
    "\n",
    "    today = datetime.today()\n",
    "    for client in client_ids:\n",
    "        if client in last_date_dict:\n",
    "            start = datetime.strptime(str(last_date_dict[client]), \"%Y-%m-%d\") + timedelta(days=1)\n",
    "        else:\n",
    "            start = datetime.strptime(bootstrap_date, \"%Y-%m-%d\")\n",
    "        dates_to_process[client] = [(start + timedelta(days=i)).strftime(\"%Y-%m-%d\") \n",
    "                                    for i in range((today - start).days + 1)]\n",
    "    return dates_to_process\n",
    "\n",
    "# ============================================================================\n",
    "# GENERALIZED DATASET PROCESSOR\n",
    "# ============================================================================\n",
    "def process_dataset(\n",
    "    raw_base_path,\n",
    "    curated_base_path,\n",
    "    client_ids,\n",
    "    dataset_name,\n",
    "    unique_keys,\n",
    "    explode_col=None,\n",
    "    dates_per_client=None\n",
    "):\n",
    "    # Entity-first curated path\n",
    "    entity_curated_path = f\"{curated_base_path}/entity={dataset_name}\"\n",
    "\n",
    "    # Required for safe incremental overwrites\n",
    "    spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "\n",
    "    records_written = 0\n",
    "\n",
    "    for client_id in client_ids:\n",
    "        client_dates = dates_per_client.get(client_id, [])\n",
    "\n",
    "        for process_date in client_dates:\n",
    "            input_path = (\n",
    "                f\"{raw_base_path}/client_id={client_id}/entity={dataset_name}/date={process_date}/\"\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                print(f\"üìÇ {dataset_name} | {client_id} | {process_date}\")\n",
    "\n",
    "                df = spark.read.parquet(input_path)\n",
    "\n",
    "                # Metadata\n",
    "                df = (\n",
    "                    df.withColumn(\"client_id\", lit(client_id))\n",
    "                      .withColumn(\"source_date\", lit(process_date))\n",
    "                      .withColumn(\"processing_timestamp\", current_timestamp())\n",
    "                      .withColumn(\"processing_date\", current_date())\n",
    "                      .withColumn(\n",
    "                          \"load_type\",\n",
    "                          lit(\"single_date\" if SINGLE_DATE else \"incremental\")\n",
    "                      )\n",
    "                )\n",
    "\n",
    "                # Explode if required\n",
    "                if explode_col:\n",
    "                    df = df.withColumn(explode_col, explode_outer(col(explode_col)))\n",
    "\n",
    "                # Deduplicate BEFORE flattening\n",
    "                df = df.dropDuplicates(unique_keys + [\"client_id\", \"source_date\"])\n",
    "\n",
    "                # Flatten nested structs\n",
    "                df = flatten_struct_columns(df)\n",
    "\n",
    "                # Write only this client+date partition\n",
    "                df.write \\\n",
    "                  .mode(\"overwrite\") \\\n",
    "                  .partitionBy(\"client_id\", \"source_date\") \\\n",
    "                  .parquet(entity_curated_path)\n",
    "\n",
    "                count = df.count()\n",
    "                records_written += count\n",
    "\n",
    "                print(f\"‚úÖ Written {count} records\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Skipped {dataset_name} | {client_id} | {process_date}: {e}\")\n",
    "\n",
    "    if records_written == 0:\n",
    "        print(f\"‚ùå No data written for {dataset_name}\")\n",
    "    else:\n",
    "        print(f\"üéØ {dataset_name} total records written: {records_written}\")\n",
    "\n",
    "# ============================================================================\n",
    "# GET DATES PER CLIENT\n",
    "# ============================================================================\n",
    "dates_per_client = get_dates_to_process(CURATED_PATH, CLIENT_IDS, single_date=SINGLE_DATE, bootstrap_date=BOOTSTRAP_DATE)\n",
    "print(dates_per_client)\n",
    "# ============================================================================\n",
    "# PROCESS ALL DATASETS\n",
    "# ============================================================================\n",
    "# activity_open\n",
    "process_dataset(RAW_PATH, CURATED_PATH, CLIENT_IDS, \"activity_open\",\n",
    "                unique_keys=[\"id\", \"recipient.id\", \"campaign.id\"], dates_per_client=dates_per_client)\n",
    "\n",
    "# activity_reply\n",
    "process_dataset(RAW_PATH, CURATED_PATH, CLIENT_IDS, \"activity_reply\",\n",
    "                unique_keys=[\"id\", \"recipient.id\", \"campaign.id\"], dates_per_client=dates_per_client)\n",
    "\n",
    "# activity_sent (explode 'to' array)\n",
    "process_dataset(RAW_PATH, CURATED_PATH, CLIENT_IDS, \"activity_sent\",\n",
    "                unique_keys=[\"id\", \"recipient.id\", \"campaign.id\"],explode_col=\"to\", dates_per_client=dates_per_client)\n",
    "\n",
    "# created_leads\n",
    "process_dataset(RAW_PATH, CURATED_PATH, CLIENT_IDS, \n",
    "                \"created_leads\",\n",
    "                unique_keys=[\"id\", \"recipient.id\", \"campaign.id\"], \n",
    "                dates_per_client=dates_per_client)\n",
    "\n",
    "# ============================================================================\n",
    "# JOB COMPLETE\n",
    "# ============================================================================\n",
    "spark.stop()\n",
    "print(\"üéâ All datasets processed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b66d7f99-5b6e-4fcb-b395-fd5a9c26b067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Single-date mode: 2025-12-25\n",
      "üìÇ activity_open | client_1 | 2025-12-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 100 records\n",
      "üìÇ activity_open | client_2 | 2025-12-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 100 records\n",
      "üìÇ activity_open | client_3 | 2025-12-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 100 records\n",
      "üìÇ activity_reply | client_1 | 2025-12-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 25 records\n",
      "üìÇ activity_reply | client_2 | 2025-12-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 25 records\n",
      "üìÇ activity_reply | client_3 | 2025-12-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 25 records\n",
      "üìÇ activity_sent | client_1 | 2025-12-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 25 records\n",
      "üìÇ activity_sent | client_2 | 2025-12-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 25 records\n",
      "üìÇ activity_sent | client_3 | 2025-12-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 25 records\n",
      "üìÇ created_leads | client_1 | 2025-12-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 100 records\n",
      "üìÇ created_leads | client_2 | 2025-12-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 43 records\n",
      "üìÇ created_leads | client_3 | 2025-12-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 16 records\n",
      "üéâ All datasets processed successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, max, lit, current_timestamp, current_date, explode_outer\n",
    ")\n",
    "from pyspark.sql.types import NullType, StringType\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# SPARK SESSION\n",
    "# ============================================================================\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"MailshakeCampaignCurations\")\n",
    "    .config(\n",
    "        \"spark.driver.extraClassPath\",\n",
    "        \"/opt/spark/jars/hadoop-aws-3.3.4.jar:/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.executor.extraClassPath\",\n",
    "        \"/opt/spark/jars/hadoop-aws-3.3.4.jar:/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# S3 CONFIG\n",
    "# ============================================================================\n",
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", os.getenv(\"AWS_ACCESS_KEY_ID\"))\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", os.getenv(\"AWS_SECRET_ACCESS_KEY\"))\n",
    "hadoop_conf.set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIG\n",
    "# ============================================================================\n",
    "RAW_PATH = \"s3a://mailshake-analytics/raw\"\n",
    "CURATED_PATH = \"s3a://mailshake-analytics/curated\"\n",
    "CLIENT_IDS = [\"client_1\", \"client_2\", \"client_3\"]\n",
    "\n",
    "SINGLE_DATE = \"2025-12-25\"\n",
    "BOOTSTRAP_DATE = \"2025-12-20\"\n",
    "\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "\n",
    "# ============================================================================\n",
    "# HELPERS\n",
    "# ============================================================================\n",
    "\n",
    "def sanitize_column_names(df):\n",
    "    \"\"\"\n",
    "    Replace dots in column names with underscores.\n",
    "    This is CRITICAL for Mailshake data.\n",
    "    \"\"\"\n",
    "    for c in df.columns:\n",
    "        if \".\" in c:\n",
    "            df = df.withColumnRenamed(c, c.replace(\".\", \"_\"))\n",
    "    return df\n",
    "\n",
    "\n",
    "def fix_void_columns(df):\n",
    "    \"\"\"\n",
    "    Cast NullType (VOID) columns so Parquet can write them.\n",
    "    \"\"\"\n",
    "    for field in df.schema.fields:\n",
    "        if isinstance(field.dataType, NullType):\n",
    "            print(f\"‚ö†Ô∏è Casting VOID column to string: {field.name}\")\n",
    "            df = df.withColumn(field.name, col(field.name).cast(StringType()))\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_dates_to_process(curated_path, client_ids, single_date=None, bootstrap_date=\"2025-12-20\"):\n",
    "    if single_date:\n",
    "        print(f\"üìÖ Single-date mode: {single_date}\")\n",
    "        return {c: [single_date] for c in client_ids}\n",
    "\n",
    "    dates = {}\n",
    "    try:\n",
    "        existing = spark.read.parquet(curated_path)\n",
    "        last_dates = (\n",
    "            existing.groupBy(\"client_id\")\n",
    "            .agg(max(\"source_date\").alias(\"last_date\"))\n",
    "            .collect()\n",
    "        )\n",
    "        last_map = {r[\"client_id\"]: r[\"last_date\"] for r in last_dates}\n",
    "    except Exception:\n",
    "        last_map = {}\n",
    "\n",
    "    today = datetime.today()\n",
    "    for client in client_ids:\n",
    "        start = (\n",
    "            datetime.strptime(str(last_map[client]), \"%Y-%m-%d\") + timedelta(days=1)\n",
    "            if client in last_map\n",
    "            else datetime.strptime(bootstrap_date, \"%Y-%m-%d\")\n",
    "        )\n",
    "        dates[client] = [\n",
    "            (start + timedelta(days=i)).strftime(\"%Y-%m-%d\")\n",
    "            for i in range((today - start).days + 1)\n",
    "        ]\n",
    "    return dates\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET PROCESSOR\n",
    "# ============================================================================\n",
    "def process_dataset(\n",
    "    raw_base_path,\n",
    "    curated_base_path,\n",
    "    client_ids,\n",
    "    dataset_name,\n",
    "    unique_keys,\n",
    "    explode_col=None,\n",
    "    dates_per_client=None\n",
    "):\n",
    "    entity_path = f\"{curated_base_path}/entity={dataset_name}\"\n",
    "\n",
    "    for client_id in client_ids:\n",
    "        for process_date in dates_per_client.get(client_id, []):\n",
    "\n",
    "            input_path = (\n",
    "                f\"{raw_base_path}/client_id={client_id}/entity={dataset_name}/date={process_date}/\"\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                print(f\"üìÇ {dataset_name} | {client_id} | {process_date}\")\n",
    "\n",
    "                df = spark.read.parquet(input_path)\n",
    "\n",
    "                # IMPORTANT: sanitize dotted columns FIRST\n",
    "                df = sanitize_column_names(df)\n",
    "\n",
    "                # Metadata\n",
    "                df = (\n",
    "                    df.withColumn(\"client_id\", lit(client_id))\n",
    "                      .withColumn(\"source_date\", lit(process_date))\n",
    "                      .withColumn(\"processing_timestamp\", current_timestamp())\n",
    "                      .withColumn(\"processing_date\", current_date())\n",
    "                      .withColumn(\n",
    "                          \"load_type\",\n",
    "                          lit(\"single_date\" if SINGLE_DATE else \"incremental\")\n",
    "                      )\n",
    "                )\n",
    "\n",
    "                # Explode if needed (use sanitized name)\n",
    "                if explode_col:\n",
    "                    explode_col = explode_col.replace(\".\", \"_\")\n",
    "                    if explode_col in df.columns:\n",
    "                        df = df.withColumn(explode_col, explode_outer(col(explode_col)))\n",
    "\n",
    "                # Deduplicate using sanitized keys\n",
    "                safe_keys = [k.replace(\".\", \"_\") for k in unique_keys]\n",
    "                df = df.dropDuplicates(safe_keys + [\"client_id\", \"source_date\"])\n",
    "\n",
    "                # Fix VOID columns\n",
    "                df = fix_void_columns(df)\n",
    "\n",
    "                # Write\n",
    "                df.write \\\n",
    "                    .mode(\"overwrite\") \\\n",
    "                    .partitionBy(\"client_id\", \"source_date\") \\\n",
    "                    .parquet(entity_path)\n",
    "\n",
    "                print(f\"‚úÖ Written {df.count()} records\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Skipped {dataset_name} | {client_id} | {process_date}: {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# RUN\n",
    "# ============================================================================\n",
    "dates_per_client = get_dates_to_process(\n",
    "    CURATED_PATH,\n",
    "    CLIENT_IDS,\n",
    "    single_date=SINGLE_DATE,\n",
    "    bootstrap_date=BOOTSTRAP_DATE\n",
    ")\n",
    "\n",
    "process_dataset(\n",
    "    RAW_PATH, CURATED_PATH, CLIENT_IDS,\n",
    "    \"activity_open\",\n",
    "    unique_keys=[\"id\", \"recipient.id\", \"campaign.id\"],\n",
    "    dates_per_client=dates_per_client\n",
    ")\n",
    "\n",
    "process_dataset(\n",
    "    RAW_PATH, CURATED_PATH, CLIENT_IDS,\n",
    "    \"activity_reply\",\n",
    "    unique_keys=[\"id\", \"recipient.id\", \"campaign.id\"],\n",
    "    dates_per_client=dates_per_client\n",
    ")\n",
    "\n",
    "process_dataset(\n",
    "    RAW_PATH, CURATED_PATH, CLIENT_IDS,\n",
    "    \"activity_sent\",\n",
    "    unique_keys=[\"id\", \"recipient.id\", \"campaign.id\"],\n",
    "    explode_col=\"to\",\n",
    "    dates_per_client=dates_per_client\n",
    ")\n",
    "\n",
    "process_dataset(\n",
    "    RAW_PATH, CURATED_PATH, CLIENT_IDS,\n",
    "    \"created_leads\",\n",
    "    unique_keys=[\"id\", \"recipient.id\", \"campaign.id\"],\n",
    "    dates_per_client=dates_per_client\n",
    ")\n",
    "\n",
    "spark.stop()\n",
    "print(\"üéâ All datasets processed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ccf2b78-707c-43f1-9ac8-839e733da471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Single-date mode enabled: 2025-12-25\n",
      "üìÇ activity_sent | client_1 | 2025-12-25\n",
      "‚ö†Ô∏è Adding missing column: recipient_fields_status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/30 23:58:58 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 25 records\n",
      "üìÇ activity_sent | client_2 | 2025-12-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 25 records\n",
      "üìÇ activity_sent | client_3 | 2025-12-25\n",
      "‚ö†Ô∏è Adding missing column: recipient_fields_status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 25 records\n",
      "üìÇ created_leads | client_1 | 2025-12-25\n",
      "‚ö†Ô∏è Adding missing column: recipient_fields_status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 100 records\n",
      "üìÇ created_leads | client_2 | 2025-12-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 43 records\n",
      "üìÇ created_leads | client_3 | 2025-12-25\n",
      "‚ö†Ô∏è Adding missing column: recipient_fields_status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 16 records\n",
      "üéâ All datasets processed successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, max, lit, current_timestamp, current_date, explode_outer\n",
    ")\n",
    "from pyspark.sql.types import NullType, StringType, StructType, DoubleType\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# SPARK SESSION\n",
    "# ============================================================================\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"MailshakeCampaignCurations\")\n",
    "    .config(\n",
    "        \"spark.driver.extraClassPath\",\n",
    "        \"/opt/spark/jars/hadoop-aws-3.3.4.jar:/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.executor.extraClassPath\",\n",
    "        \"/opt/spark/jars/hadoop-aws-3.3.4.jar:/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "\n",
    "# ============================================================================\n",
    "# S3 CONFIG\n",
    "# ============================================================================\n",
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", os.getenv(\"AWS_ACCESS_KEY_ID\"))\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", os.getenv(\"AWS_SECRET_ACCESS_KEY\"))\n",
    "hadoop_conf.set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIG\n",
    "# ============================================================================\n",
    "RAW_PATH = \"s3a://mailshake-analytics/raw\"\n",
    "CURATED_PATH = \"s3a://mailshake-analytics/curated\"\n",
    "CLIENT_IDS = [\"client_1\", \"client_2\", \"client_3\"]\n",
    "\n",
    "SINGLE_DATE = \"2025-12-25\"   # set None for incremental\n",
    "BOOTSTRAP_DATE = \"2025-12-20\"\n",
    "\n",
    "# ============================================================================\n",
    "# HELPERS\n",
    "# ============================================================================\n",
    "\n",
    "def sanitize_columns(df):\n",
    "    for col_name in df.columns:\n",
    "        clean = re.sub(r'[^a-zA-Z0-9_]', '_', col_name)\n",
    "        clean = re.sub(r'_+', '_', clean).lower()\n",
    "        if clean != col_name:\n",
    "            df = df.withColumnRenamed(col_name, clean)\n",
    "    return df\n",
    "\n",
    "\n",
    "def fix_void_columns(df):\n",
    "    \"\"\"Cast NullType columns so Parquet can write.\"\"\"\n",
    "    for field in df.schema.fields:\n",
    "        if isinstance(field.dataType, NullType):\n",
    "            df = df.withColumn(field.name, col(field.name).cast(StringType()))\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_explicit_missing_columns(df, required_columns):\n",
    "    \"\"\"\n",
    "    Add explicitly required columns if missing.\n",
    "    required_columns = { \"column_name\": DataType }\n",
    "    \"\"\"\n",
    "    for col_name, data_type in required_columns.items():\n",
    "        if col_name not in df.columns:\n",
    "            print(f\"‚ö†Ô∏è Adding missing column: {col_name}\")\n",
    "            df = df.withColumn(col_name, lit(None).cast(data_type))\n",
    "    return df\n",
    "\n",
    "def flatten_struct_columns(df):\n",
    "    while True:\n",
    "        struct_cols = [\n",
    "            field.name\n",
    "            for field in df.schema.fields\n",
    "            if isinstance(field.dataType, StructType)\n",
    "        ]\n",
    "\n",
    "        if not struct_cols:\n",
    "            break\n",
    "\n",
    "        for col_name in struct_cols:\n",
    "            for nested in df.schema[col_name].dataType.fields:\n",
    "                df = df.withColumn(\n",
    "                    f\"{col_name}_{nested.name}\",\n",
    "                    col(f\"{col_name}.{nested.name}\")\n",
    "                )\n",
    "            df = df.drop(col_name)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_dates_to_process(curated_path, client_ids, single_date=None, bootstrap_date=\"2025-12-20\"):\n",
    "    if single_date:\n",
    "        print(f\"üìÖ Single-date mode enabled: {single_date}\")\n",
    "        return {c: [single_date] for c in client_ids}\n",
    "\n",
    "    dates = {}\n",
    "    try:\n",
    "        existing = spark.read.parquet(curated_path)\n",
    "        last_dates = (\n",
    "            existing.groupBy(\"client_id\")\n",
    "            .agg(max(\"source_date\").alias(\"last_date\"))\n",
    "            .collect()\n",
    "        )\n",
    "        last_map = {r[\"client_id\"]: r[\"last_date\"] for r in last_dates}\n",
    "    except Exception:\n",
    "        last_map = {}\n",
    "\n",
    "    today = datetime.today()\n",
    "    for client in client_ids:\n",
    "        start = (\n",
    "            datetime.strptime(str(last_map[client]), \"%Y-%m-%d\") + timedelta(days=1)\n",
    "            if client in last_map\n",
    "            else datetime.strptime(bootstrap_date, \"%Y-%m-%d\")\n",
    "        )\n",
    "        dates[client] = [\n",
    "            (start + timedelta(days=i)).strftime(\"%Y-%m-%d\")\n",
    "            for i in range((today - start).days + 1)\n",
    "        ]\n",
    "    return dates\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET PROCESSOR\n",
    "# ============================================================================\n",
    "def process_dataset(\n",
    "    raw_base_path,\n",
    "    curated_base_path,\n",
    "    client_ids,\n",
    "    dataset_name,\n",
    "    unique_keys,\n",
    "    explode_col=None,\n",
    "    dates_per_client=None,\n",
    "    required_columns=None\n",
    "):\n",
    "    entity_path = f\"{curated_base_path}/entity={dataset_name}\"\n",
    "\n",
    "    for client_id in client_ids:\n",
    "        for process_date in dates_per_client.get(client_id, []):\n",
    "\n",
    "            input_path = (\n",
    "                f\"{raw_base_path}/client_id={client_id}/entity={dataset_name}/date={process_date}/\"\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                print(f\"üìÇ {dataset_name} | {client_id} | {process_date}\")\n",
    "\n",
    "                df = spark.read.parquet(input_path)\n",
    "\n",
    "                df = flatten_struct_columns(df)   # ‚úÖ FLATTEN FIRST\n",
    "                \n",
    "                # üö´ explode only if column is ARRAY\n",
    "                if explode_col:\n",
    "                    explode_col = explode_col.replace(\".\", \"_\")\n",
    "                    if explode_col in df.columns:\n",
    "                        df = df.withColumn(explode_col, explode_outer(col(explode_col)))\n",
    "                \n",
    "                df = sanitize_column_names(df)    # sanitize again after flatten\n",
    "\n",
    "                # Metadata\n",
    "                df = (\n",
    "                    df.withColumn(\"client_id\", lit(client_id))\n",
    "                      .withColumn(\"source_date\", lit(process_date))\n",
    "                       .withColumn(\"client_id_col\", lit(client_id))        # physical copy\n",
    "                      .withColumn(\"source_date_col\", lit(process_date))   # physical copy\n",
    "                      .withColumn(\"processing_timestamp\", current_timestamp())\n",
    "                      .withColumn(\"processing_date\", current_date())\n",
    "                      .withColumn(\n",
    "                          \"load_type\",\n",
    "                          lit(\"single_date\" if SINGLE_DATE else \"incremental\")\n",
    "                      )\n",
    "                )\n",
    "\n",
    "                # Explicit missing columns\n",
    "                if required_columns:\n",
    "                    df = add_explicit_missing_columns(df, required_columns)\n",
    "\n",
    "                # Deduplicate\n",
    "                safe_keys = [k.replace(\".\", \"_\") for k in unique_keys]\n",
    "                df = df.dropDuplicates(safe_keys + [\"client_id\", \"source_date\"])\n",
    "\n",
    "                # Fix VOID columns\n",
    "                df = fix_void_columns(df)\n",
    "\n",
    "                # Write\n",
    "                df.write \\\n",
    "                    .mode(\"overwrite\") \\\n",
    "                    .partitionBy(\"client_id\", \"source_date\") \\\n",
    "                    .parquet(entity_path)\n",
    "\n",
    "                print(f\"‚úÖ Written {df.count()} records\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Skipped {dataset_name} | {client_id} | {process_date}: {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# RUN\n",
    "# ============================================================================\n",
    "dates_per_client = get_dates_to_process(\n",
    "    CURATED_PATH,\n",
    "    CLIENT_IDS,\n",
    "    single_date=SINGLE_DATE,\n",
    "    bootstrap_date=BOOTSTRAP_DATE\n",
    ")\n",
    "\n",
    "# # -------------------- activity_open --------------------\n",
    "# process_dataset(\n",
    "#     RAW_PATH,\n",
    "#     CURATED_PATH,\n",
    "#     CLIENT_IDS,\n",
    "#     \"activity_open\",\n",
    "#     unique_keys=[\"id\", \"recipient.id\", \"campaign.id\"],\n",
    "#     dates_per_client=dates_per_client,\n",
    "#     required_columns={\n",
    "#         \"recipient_fields_status\": StringType()\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# # -------------------- activity_reply --------------------\n",
    "# process_dataset(\n",
    "#     RAW_PATH,\n",
    "#     CURATED_PATH,\n",
    "#     CLIENT_IDS,\n",
    "#     \"activity_reply\",\n",
    "#     unique_keys=[\"id\", \"recipient.id\", \"campaign.id\"],\n",
    "#     dates_per_client=dates_per_client,\n",
    "#     required_columns={\n",
    "#         \"recipient_fields_status\": StringType()\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# -------------------- activity_sent --------------------\n",
    "process_dataset(\n",
    "    RAW_PATH,\n",
    "    CURATED_PATH,\n",
    "    CLIENT_IDS,\n",
    "    \"activity_sent\",\n",
    "    unique_keys=[\"id\", \"recipient.id\", \"campaign.id\"],\n",
    "    explode_col=None,\n",
    "    dates_per_client=dates_per_client,\n",
    "    required_columns={\n",
    "        \"recipient_fields_status\": StringType()\n",
    "    }\n",
    ")\n",
    "\n",
    "# -------------------- created_leads --------------------\n",
    "process_dataset(\n",
    "    RAW_PATH,\n",
    "    CURATED_PATH,\n",
    "    CLIENT_IDS,\n",
    "    \"created_leads\",\n",
    "    unique_keys=[\"id\", \"recipient.id\", \"campaign.id\"],\n",
    "    dates_per_client=dates_per_client,\n",
    "    required_columns={\n",
    "        \"recipient_fields_status\": StringType()\n",
    "    }\n",
    ")\n",
    "\n",
    "spark.stop()\n",
    "print(\"üéâ All datasets processed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc24939d-e715-4700-89a0-13cb4a89183e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Single-date mode enabled: 2025-12-25\n",
      "üìÇ activity_sent | client_1 | 2025-12-25\n",
      "‚ö†Ô∏è Adding missing column: recipient_fields_status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 25 records\n",
      "üìÇ activity_sent | client_2 | 2025-12-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 25 records\n",
      "üìÇ activity_sent | client_3 | 2025-12-25\n",
      "‚ö†Ô∏è Adding missing column: recipient_fields_status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 25 records\n",
      "üìÇ created_leads | client_1 | 2025-12-25\n",
      "‚ö†Ô∏è Adding missing column: recipient_fields_first\n",
      "‚ö†Ô∏è Adding missing column: recipient_fields_status\n",
      "‚ö†Ô∏è Adding missing column: assignedto_object\n",
      "‚ö†Ô∏è Adding missing column: assignedto_id\n",
      "‚ö†Ô∏è Adding missing column: assignedto_emailaddress\n",
      "‚ö†Ô∏è Adding missing column: assignedto_fullname\n",
      "‚ö†Ô∏è Adding missing column: assignedto_first\n",
      "‚ö†Ô∏è Adding missing column: assignedto_last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 100 records\n",
      "üìÇ created_leads | client_2 | 2025-12-25\n",
      "‚ö†Ô∏è Adding missing column: recipient_fields_first\n",
      "‚ö†Ô∏è Adding missing column: assignedto_object\n",
      "‚ö†Ô∏è Adding missing column: assignedto_id\n",
      "‚ö†Ô∏è Adding missing column: assignedto_emailaddress\n",
      "‚ö†Ô∏è Adding missing column: assignedto_fullname\n",
      "‚ö†Ô∏è Adding missing column: assignedto_first\n",
      "‚ö†Ô∏è Adding missing column: assignedto_last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 43 records\n",
      "üìÇ created_leads | client_3 | 2025-12-25\n",
      "‚ö†Ô∏è Adding missing column: recipient_fields_status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 16 records\n",
      "üéâ All datasets processed successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col, max, lit, current_timestamp, current_date, explode_outer\n",
    ")\n",
    "from pyspark.sql.types import NullType, StringType, StructType, ArrayType\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ============================================================================\n",
    "# SPARK SESSION\n",
    "# ============================================================================\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"MailshakeCampaignCurations\")\n",
    "    .config(\n",
    "        \"spark.driver.extraClassPath\",\n",
    "        \"/opt/spark/jars/hadoop-aws-3.3.4.jar:/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.executor.extraClassPath\",\n",
    "        \"/opt/spark/jars/hadoop-aws-3.3.4.jar:/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "\n",
    "# ============================================================================\n",
    "# S3 CONFIG\n",
    "# ============================================================================\n",
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", os.getenv(\"AWS_ACCESS_KEY_ID\"))\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", os.getenv(\"AWS_SECRET_ACCESS_KEY\"))\n",
    "hadoop_conf.set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIG\n",
    "# ============================================================================\n",
    "RAW_PATH = \"s3a://mailshake-analytics/raw\"\n",
    "CURATED_PATH = \"s3a://mailshake-analytics/curated\"\n",
    "CLIENT_IDS = [\"client_1\", \"client_2\", \"client_3\"]\n",
    "\n",
    "SINGLE_DATE = \"2025-12-25\"   # set None for incremental\n",
    "BOOTSTRAP_DATE = \"2025-12-20\"\n",
    "\n",
    "# ============================================================================\n",
    "# HELPERS\n",
    "# ============================================================================\n",
    "\n",
    "def sanitize_column_names(df):\n",
    "    \"\"\"Replace invalid characters and dots in column names with underscores\"\"\"\n",
    "    for col_name in df.columns:\n",
    "        clean = re.sub(r'[^a-zA-Z0-9_]', '_', col_name)\n",
    "        clean = re.sub(r'_+', '_', clean).lower()\n",
    "        if clean != col_name:\n",
    "            df = df.withColumnRenamed(col_name, clean)\n",
    "    return df\n",
    "\n",
    "def fix_void_columns(df):\n",
    "    \"\"\"Cast NullType columns so Parquet can write.\"\"\"\n",
    "    for field in df.schema.fields:\n",
    "        if isinstance(field.dataType, NullType):\n",
    "            df = df.withColumn(field.name, col(field.name).cast(StringType()))\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def ensure_columns_and_reorder(df: DataFrame, column_order: list, column_types: dict = None) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure all columns exist (with specified data types if provided) and reorder DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): input DataFrame\n",
    "        column_order (list): list of columns in the desired order\n",
    "        column_types (dict, optional): dictionary of {column_name: DataType} for missing columns\n",
    "    Returns:\n",
    "        DataFrame with all columns present and reordered\n",
    "    \"\"\"\n",
    "    column_types = column_types or {}\n",
    "\n",
    "    for col_name in column_order:\n",
    "        if col_name not in df.columns:\n",
    "            dtype = column_types.get(col_name)\n",
    "            if dtype:\n",
    "                df = df.withColumn(col_name, lit(None).cast(dtype))\n",
    "            else:\n",
    "                df = df.withColumn(col_name, lit(None))\n",
    "            print(f\"‚ö†Ô∏è Adding missing column: {col_name}\")\n",
    "\n",
    "    # Select columns in the desired order\n",
    "    df = df.select([col(c) for c in column_order])\n",
    "    return df\n",
    "\n",
    "\n",
    "def flatten_struct_columns(df):\n",
    "    \"\"\"Flatten all StructType columns and explode arrays of structs.\"\"\"\n",
    "    while True:\n",
    "        struct_cols = [\n",
    "            field.name\n",
    "            for field in df.schema.fields\n",
    "            if isinstance(field.dataType, StructType)\n",
    "        ]\n",
    "\n",
    "        if not struct_cols:\n",
    "            break\n",
    "\n",
    "        for col_name in struct_cols:\n",
    "            for nested in df.schema[col_name].dataType.fields:\n",
    "                df = df.withColumn(\n",
    "                    f\"{col_name}_{nested.name}\",\n",
    "                    col(f\"{col_name}.{nested.name}\")\n",
    "                )\n",
    "            df = df.drop(col_name)\n",
    "\n",
    "    # Explode array<struct> columns\n",
    "    array_struct_cols = [\n",
    "        field.name\n",
    "        for field in df.schema.fields\n",
    "        if isinstance(field.dataType, ArrayType) and isinstance(field.dataType.elementType, StructType)\n",
    "    ]\n",
    "\n",
    "    for col_name in array_struct_cols:\n",
    "        df = df.withColumn(col_name, explode_outer(col(col_name)))\n",
    "        for nested in df.schema[col_name].dataType.fields:\n",
    "            df = df.withColumn(f\"{col_name}_{nested.name}\", col(f\"{col_name}.{nested.name}\"))\n",
    "        df = df.drop(col_name)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_dates_to_process(curated_path, client_ids, single_date=None, bootstrap_date=\"2025-12-20\"):\n",
    "    if single_date:\n",
    "        print(f\"üìÖ Single-date mode enabled: {single_date}\")\n",
    "        return {c: [single_date] for c in client_ids}\n",
    "\n",
    "    dates = {}\n",
    "    try:\n",
    "        existing = spark.read.parquet(curated_path)\n",
    "        last_dates = (\n",
    "            existing.groupBy(\"client_id\")\n",
    "            .agg(max(\"source_date\").alias(\"last_date\"))\n",
    "            .collect()\n",
    "        )\n",
    "        last_map = {r[\"client_id\"]: r[\"last_date\"] for r in last_dates}\n",
    "    except Exception:\n",
    "        last_map = {}\n",
    "\n",
    "    today = datetime.today()\n",
    "    for client in client_ids:\n",
    "        start = (\n",
    "            datetime.strptime(str(last_map[client]), \"%Y-%m-%d\") + timedelta(days=1)\n",
    "            if client in last_map\n",
    "            else datetime.strptime(bootstrap_date, \"%Y-%m-%d\")\n",
    "        )\n",
    "        dates[client] = [\n",
    "            (start + timedelta(days=i)).strftime(\"%Y-%m-%d\")\n",
    "            for i in range((today - start).days + 1)\n",
    "        ]\n",
    "    return dates\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET PROCESSOR\n",
    "# ============================================================================\n",
    "def process_dataset(\n",
    "    raw_base_path,\n",
    "    curated_base_path,\n",
    "    client_ids,\n",
    "    dataset_name,\n",
    "    unique_keys,\n",
    "    explode_col=None,\n",
    "    dates_per_client=None,\n",
    "    desired_columns =None,\n",
    "    column_types=None\n",
    "):\n",
    "    entity_path = f\"{curated_base_path}/entity={dataset_name}\"\n",
    "\n",
    "    for client_id in client_ids:\n",
    "        for process_date in dates_per_client.get(client_id, []):\n",
    "            input_path = (\n",
    "                f\"{raw_base_path}/client_id={client_id}/entity={dataset_name}/date={process_date}/\"\n",
    "            )\n",
    "            try:\n",
    "                print(f\"üìÇ {dataset_name} | {client_id} | {process_date}\")\n",
    "\n",
    "                df = spark.read.parquet(input_path)\n",
    "\n",
    "                df = flatten_struct_columns(df)   # flatten structs and arrays first\n",
    "\n",
    "                if explode_col:\n",
    "                    explode_col = explode_col.replace(\".\", \"_\")\n",
    "                    if explode_col in df.columns:\n",
    "                        df = df.withColumn(explode_col, explode_outer(col(explode_col)))\n",
    "\n",
    "                # Sanitize column names\n",
    "                df = sanitize_column_names(df)\n",
    "\n",
    "                # Ensure all columns exist and reorder\n",
    "                df = ensure_columns_and_reorder(df, desired_columns,column_types)\n",
    "\n",
    "                # Add metadata\n",
    "                df = (\n",
    "                    df.withColumn(\"client_id\", lit(client_id))\n",
    "                      .withColumn(\"source_date\", lit(process_date))\n",
    "                      .withColumn(\"client_id_col\", lit(client_id))\n",
    "                      .withColumn(\"source_date_col\", lit(process_date))\n",
    "                      .withColumn(\"processing_timestamp\", current_timestamp())\n",
    "                      .withColumn(\"processing_date\", current_date())\n",
    "                      .withColumn(\"load_type\", lit(\"single_date\" if SINGLE_DATE else \"incremental\"))\n",
    "                )\n",
    "\n",
    "               # # Add missing columns\n",
    "               # if required_columns:\n",
    "                #    df = add_explicit_missing_columns(df, required_columns)\n",
    "\n",
    "                # Deduplicate\n",
    "                safe_keys = [k.replace(\".\", \"_\") for k in unique_keys]\n",
    "                df = df.dropDuplicates(safe_keys + [\"client_id\", \"source_date\"])\n",
    "\n",
    "                df = fix_void_columns(df)\n",
    "\n",
    "                # Write\n",
    "                df.write.mode(\"overwrite\").partitionBy(\"client_id\", \"source_date\").parquet(entity_path)\n",
    "                print(f\"‚úÖ Written {df.count()} records\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Skipped {dataset_name} | {client_id} | {process_date}: {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# RUN\n",
    "# ============================================================================\n",
    "dates_per_client = get_dates_to_process(CURATED_PATH, CLIENT_IDS, single_date=SINGLE_DATE, bootstrap_date=BOOTSTRAP_DATE)\n",
    "\n",
    "# # -------------------- activity_open --------------------\n",
    "# process_dataset(\n",
    "#     RAW_PATH, CURATED_PATH, CLIENT_IDS,\n",
    "#     \"activity_open\",\n",
    "#     unique_keys=[\"id\", \"recipient.id\", \"campaign.id\"],\n",
    "#     dates_per_client=dates_per_client,\n",
    "#     desired_columns = [\n",
    "#             \"object\", \"id\", \"actiondate\", \"isduplicate\", \"recipient_object\",\n",
    "#             \"recipient_id\", \"recipient_emailaddress\", \"recipient_fullname\",\n",
    "#             \"recipient_created\", \"recipient_ispaused\", \"recipient_contactid\",\n",
    "#             \"recipient_first\", \"recipient_last\", \"recipient_fields_link\",\n",
    "#             \"recipient_fields_status\", \"recipient_fields_first\",\n",
    "#             \"recipient_fields_position\", \"recipient_fields_date_applied\",\n",
    "#             \"recipient_fields_account\", \"recipient_fields_phonenumber\",\n",
    "#             \"recipient_fields_facebookurl\", \"recipient_fields_instagramid\",\n",
    "#             \"recipient_fields_linkedinurl\", \"recipient_fields_twitterid\",\n",
    "#             \"campaign_object\", \"campaign_id\", \"campaign_title\", \"campaign_wizardstatus\",\n",
    "#             \"parent_object\", \"parent_id\", \"parent_type\", \"parent_message_object\",\n",
    "#             \"parent_message_id\", \"parent_message_type\", \"parent_message_subject\",\n",
    "#             \"parent_message_replytoid\"\n",
    "#         ],\n",
    "#             # Optional: specify data types for missing columns\n",
    "#         column_types = {\n",
    "#             \"recipient_fields_status\": StringType(),\n",
    "#             \"recipient_fields_first\": StringType()\n",
    "#         }\n",
    "# )\n",
    "\n",
    "# # -------------------- activity_reply --------------------\n",
    "# process_dataset(\n",
    "#     RAW_PATH, CURATED_PATH, CLIENT_IDS,\n",
    "#     \"activity_reply\",\n",
    "#     unique_keys=[\"id\", \"recipient.id\", \"campaign.id\"],\n",
    "#     dates_per_client=dates_per_client,\n",
    "#     desired_columns = [\n",
    "#             \"object\", \"id\", \"actiondate\",\n",
    "#             \"type\", \"subject\", \"externalid\",\n",
    "#             \"externalrawmessageid\", \"externalconversationid\", \"rawbody\",\n",
    "#             \"body\", \"plaintextbody\", \"recipient_object\",\n",
    "#             \"recipient_id\", \"recipient_emailaddress\", \"recipient_fullname\",\n",
    "#             \"recipient_created\", \"recipient_ispaused\", \"recipient_contactid\",\n",
    "#             \"recipient_first\", \"recipient_last\", \"recipient_fields_link\",\n",
    "#             \"recipient_fields_status\",  \"recipient_fields_first\", \"recipient_fields_position\", \"recipient_fields_date_applied\",\n",
    "#             \"recipient_fields_account\", \"recipient_fields_phonenumber\", \"recipient_fields_facebookurl\",\n",
    "#             \"recipient_fields_instagramid\", \"recipient_fields_linkedinurl\", \"recipient_fields_twitterid\",\n",
    "#             \"campaign_object\", \"campaign_id\", \"campaign_title\",\n",
    "#             \"campaign_wizardstatus\", \"parent_object\", \"parent_id\",\n",
    "#             \"parent_type\", \"parent_message_object\", \"parent_message_id\",\n",
    "#             \"parent_message_type\", \"parent_message_subject\", \"parent_message_replytoid\",\n",
    "#             \"from_object\", \"from_address\", \"from_fullname\",\n",
    "#             \"from_first\", \"from_last\"\n",
    "#             ],\n",
    "#     column_types ={\"recipient_fields_status\": StringType(),\n",
    "#                      \"recipient_fields_first\": StringType()}\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------- activity_sent --------------------\n",
    "process_dataset(\n",
    "    RAW_PATH, CURATED_PATH, CLIENT_IDS,\n",
    "    \"activity_sent\",\n",
    "    unique_keys=[\"id\", \"recipient.id\", \"campaign.id\"],\n",
    "    explode_col=\"to\",\n",
    "    dates_per_client=dates_per_client,\n",
    "    desired_columns =\n",
    "                    # Core\n",
    "                    [\"object\", \"id\", \"actiondate\",\n",
    "                    \"type\", \"excludebody\",\n",
    "                    # To (exploded)\n",
    "                    \"to_address\", \"to_first\", \"to_fullname\",\n",
    "                    \"to_last\", \"to_object\",\n",
    "                    # Message content\n",
    "                    \"subject\", \"externalid\", \"externalrawmessageid\",\n",
    "                    \"externalconversationid\", \"rawbody\", \"body\",\n",
    "                    \"plaintextbody\",\n",
    "                    # Recipient\n",
    "                    \"recipient_object\", \"recipient_id\", \"recipient_emailaddress\",\n",
    "                    \"recipient_fullname\", \"recipient_created\", \"recipient_ispaused\",\n",
    "                    \"recipient_first\", \"recipient_last\",\n",
    "                    # Recipient fields\n",
    "                    \"recipient_fields_account\", \"recipient_fields_phonenumber\",\n",
    "                    \"recipient_fields_facebookurl\", \"recipient_fields_instagramid\",\n",
    "                    \"recipient_fields_linkedinurl\", \"recipient_fields_twitterid\",\n",
    "                    \"recipient_fields_link\", \"recipient_fields_position\",\n",
    "                    \"recipient_fields_date_applied\", \"recipient_fields_status\",\n",
    "                    # Campaign\n",
    "                    \"campaign_object\", \"campaign_id\",\n",
    "                    \"campaign_title\", \"campaign_wizardstatus\",\n",
    "                    # Message (parent)\n",
    "                    \"message_object\", \"message_id\", \"message_type\",\n",
    "                    \"message_subject\", \"message_replytoid\",\n",
    "                    # From\n",
    "                    \"from_object\", \"from_address\", \"from_fullname\",\n",
    "                    \"from_first\", \"from_last\"],\n",
    "      column_types={\"recipient_fields_status\": StringType()}\n",
    ")\n",
    "\n",
    "# -------------------- created_leads --------------------\n",
    "process_dataset(\n",
    "    RAW_PATH, CURATED_PATH, CLIENT_IDS,\n",
    "    \"created_leads\",\n",
    "    unique_keys=[\"id\", \"recipient.id\", \"campaign.id\"],\n",
    "    dates_per_client=dates_per_client,\n",
    "    desired_columns = [\n",
    "    \"object\", \"id\", \"created\",\n",
    "    \"openeddate\", \"laststatuschangedate\", \"annotation\",\n",
    "    \"status\",\n",
    "\n",
    "    \"recipient_object\", \"recipient_id\", \"recipient_emailaddress\",\n",
    "    \"recipient_fullname\", \"recipient_created\", \"recipient_ispaused\",\n",
    "    \"recipient_contactid\", \"recipient_first\", \"recipient_last\",\n",
    "\n",
    "    \"recipient_fields_link\", \"recipient_fields_first\",\n",
    "    \"recipient_fields_status\", \"recipient_fields_position\",\n",
    "    \"recipient_fields_date_applied\", \"recipient_fields_account\",\n",
    "    \"recipient_fields_phonenumber\", \"recipient_fields_facebookurl\",\n",
    "    \"recipient_fields_instagramid\", \"recipient_fields_linkedinurl\",\n",
    "    \"recipient_fields_twitterid\",\n",
    "\n",
    "    \"campaign_object\", \"campaign_id\",\n",
    "    \"campaign_title\", \"campaign_wizardstatus\",\n",
    "\n",
    "    \"assignedto_object\", \"assignedto_id\",\n",
    "    \"assignedto_emailaddress\", \"assignedto_fullname\",\n",
    "    \"assignedto_first\", \"assignedto_last\"\n",
    "]       ,\n",
    "    column_types={  \"recipient_fields_status\": StringType(),\n",
    "                    \"recipient_fields_first\": StringType(),\n",
    "                    \"assignedto_object\": StringType(),\n",
    "                    \"assignedto_id\": DoubleType(),\n",
    "                    \"assignedto_emailaddress\": StringType(),\n",
    "                    \"assignedto_fullname\": StringType(),\n",
    "                    \"assignedto_first\": StringType(),\n",
    "                    \"assignedto_last\": StringType()\n",
    "}\n",
    ")\n",
    "\n",
    "spark.stop()\n",
    "print(\"üéâ All datasets processed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21099e77-5340-42ab-a351-7d0cb0bdb8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|recipient_fields_link                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|https://careers.hrblock.com/jobs/28304                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|https://careers.thrivent.com/jobs/44182/analyst-campaign-analytics/?source=ORG_LI                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|https://jobs.seattleymca.org/jobs/17600?lang=en-us&mode=job&iis=LinkedIn&iisn=LinkedIn.com                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|https://careers.foundationfinance.com/jobs/1798/Data%20Analyst%20II?lang=en-US&mode=apply&iis=LinkedIn                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|https://buildertrend.wd108.myworkdayjobs.com/External_Careers/job/Buildertrend-Home-Office/Senior-Data-Analyst_JR-000076?source=LinkedIn                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|https://www.linkedin.com/jobs/view/4303807967/?alternateChannel=search&eBP=CwEAAAGZdk0yJn9i_D7S_IfP-FIraN2wlQNu88fOnKzqZnHwCqottPc2PboUlUYX4GucolxDKg5_FGx92T821XKbB_fboYfJYIP3X0kWmWbc20hwBz_g6rrYuqMMvZ3Fgzv-CEvgHVVD4TWFMDsFtFEbfjBk3reAOB7gwCvrMGpQIc4uDSfp31OOUMGbdWyPvvJROFuTybKF3c5ovsY3LRU72_QztV2ie44VZUnMny8Jo9r-Jk7rI4CisPCleFuM5788wXqHVW1YJHrTctmVQ7qIrnROH2k99RmcLmpCcl9iupA1vpBiep_gKbSybGRruWOuiR4BQSqGufcMfNcqqB4r06Zc2fCW4BaOYE-HazD9nFdN7K0ZMWahUJpjh3mK4aN2G3le9lahfoExEDjFZvSSXQquft2S_rk6_rxmRW3smqvXhjTO1sVEMQZBhPZZRn4E6I04bNTwXYn963IhGByPVsnaaO4XufuUvJpfeYEryMM_s14qXraGGzwVOzzsgDg6V6akls4&refId=ERlLb7AGf3LLzQ1M85lLVg%3D%3D&trackingId=E2sZcMQdsMW5cL18JcTT%2Bg%3D%3D|\n",
      "|https://hdsupply.jobs/us/en/job/R25004597/Data-Analytics-Specialist                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|https://massgeneralbrigham.wd1.myworkdayjobs.com/MGBExternal/job/Somerville-MA/Data-Analyst_RQ4036902?source=jobright                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|https://boards.greenhouse.io/embed/job_app?token=8201916002&utm_source=jobright                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|https://nascentiahealth.hrmdirect.com/employment/job-opening.php?req=3545207&req_loc=1071536&&cust_sort1=195638&&_ga=2.204801752.910152818.1760095341-157182829.1760095341                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|https://ewaketalent.csod.com/ux/ats/careersite/3/home/requisition/8785?c=ewaketalent                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|https://boards.greenhouse.io/embed/job_app?token=7498796003&utm_source=jobright                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|https://hyvee.wd1.myworkdayjobs.com/HyVeeCareers/job/Helpful-Smiles-Technology-Center-HST-James-Street-Grimes-IA/Associate-Data-Analyst_R200828                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|https://www.linkedin.com/jobs/search/?currentJobId=4318371864&distance=25&f_E=2%2C3%2C4&f_SB2=3&f_WT=1%2C3&geoId=103644278&keywords=Data%20analyst&origin=JOB_SEARCH_PAGE_JOB_FILTER&refresh=true&start=75                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|https://www.lifeatspotify.com/jobs/analytics-engineer-business-strategy-insights                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "|https://fa-euxy-saasfaprod1.fa.ocs.oraclecloud.com/hcmUI/CandidateExperience/en/sites/CX_1/job/925                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|https://careers.dc.gov/psc/erecruit/EMPLOYEE/HRMS/c/HRS_HRAM_FL.HRS_CG_SEARCH_FL.GBL?Page=HRS_APP_JBPST_FL&Action=U&FOCUS=Applicant&SiteId=1&JobOpeningId=30743&PostingSeq=1&                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|https://job-boards.greenhouse.io/calm/jobs/8167807002?utm_source=Otta                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|https://careers.mymichigan.org/jobs/42977                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|https://job-boards.greenhouse.io/calm/jobs/8167807002?utm_source=Otta                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Create Spark session with S3 configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"JupyterS3\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/opt/spark/jars/hadoop-aws-3.3.4.jar:/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar\") \\\n",
    "    .config(\"spark.executor.extraClassPath\", \"/opt/spark/jars/hadoop-aws-3.3.4.jar:/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configure S3\n",
    "aws_access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", aws_access_key)\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", aws_secret_key)\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "df1 = spark.read.parquet(\n",
    "    \"s3a://mailshake-analytics/curated/entity=created_leads/client_id=client_2/source_date=2025-12-25/\")\n",
    "\n",
    "print(len(df1.columns))\n",
    "df1.select(\"recipient_fields_link\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14e51d21-7db1-4b87-bff6-cefa83f5c66c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing in DF2: set()\n",
      "Missing in DF1: set()\n",
      "42\n",
      "+-----------------------+\n",
      "|recipient_fields_status|\n",
      "+-----------------------+\n",
      "|                   NULL|\n",
      "|                   NULL|\n",
      "|                   NULL|\n",
      "|                   NULL|\n",
      "|                   NULL|\n",
      "|                   NULL|\n",
      "|                   NULL|\n",
      "|                   NULL|\n",
      "|                   NULL|\n",
      "|                   NULL|\n",
      "|                   NULL|\n",
      "|                   NULL|\n",
      "|                   NULL|\n",
      "|                   NULL|\n",
      "|                   NULL|\n",
      "|                   NULL|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.parquet(\n",
    "    \"s3a://mailshake-analytics/curated/entity=created_leads/client_id=client_3/source_date=2025-12-25/\")\n",
    "\n",
    "# Quick check\n",
    "print(f\"Missing in DF2: {set(df1.columns) - set(df2.columns)}\")\n",
    "print(f\"Missing in DF1: {set(df2.columns) - set(df1.columns)}\")\n",
    "\n",
    "print(len(df2.columns))\n",
    "df2.select(\"recipient_fields_status\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa2bf4b6-390c-46b0-bcc7-016195d20ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing in DF2: set()\n",
      "Missing in DF1: set()\n",
      "Missing in DF3: set()\n",
      "Missing in DF3: set()\n",
      "42\n",
      "root\n",
      " |-- object: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- created: string (nullable = true)\n",
      " |-- openeddate: string (nullable = true)\n",
      " |-- laststatuschangedate: integer (nullable = true)\n",
      " |-- annotation: integer (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- recipient_object: string (nullable = true)\n",
      " |-- recipient_id: long (nullable = true)\n",
      " |-- recipient_emailaddress: string (nullable = true)\n",
      " |-- recipient_fullname: string (nullable = true)\n",
      " |-- recipient_created: string (nullable = true)\n",
      " |-- recipient_ispaused: boolean (nullable = true)\n",
      " |-- recipient_contactid: long (nullable = true)\n",
      " |-- recipient_first: string (nullable = true)\n",
      " |-- recipient_last: string (nullable = true)\n",
      " |-- recipient_fields_link: string (nullable = true)\n",
      " |-- recipient_fields_first: string (nullable = true)\n",
      " |-- recipient_fields_status: string (nullable = true)\n",
      " |-- recipient_fields_position: string (nullable = true)\n",
      " |-- recipient_fields_date_applied: string (nullable = true)\n",
      " |-- recipient_fields_account: string (nullable = true)\n",
      " |-- recipient_fields_phonenumber: string (nullable = true)\n",
      " |-- recipient_fields_facebookurl: string (nullable = true)\n",
      " |-- recipient_fields_instagramid: string (nullable = true)\n",
      " |-- recipient_fields_linkedinurl: string (nullable = true)\n",
      " |-- recipient_fields_twitterid: string (nullable = true)\n",
      " |-- campaign_object: string (nullable = true)\n",
      " |-- campaign_id: long (nullable = true)\n",
      " |-- campaign_title: string (nullable = true)\n",
      " |-- campaign_wizardstatus: integer (nullable = true)\n",
      " |-- assignedto_object: string (nullable = true)\n",
      " |-- assignedto_id: double (nullable = true)\n",
      " |-- assignedto_emailaddress: string (nullable = true)\n",
      " |-- assignedto_fullname: string (nullable = true)\n",
      " |-- assignedto_first: string (nullable = true)\n",
      " |-- assignedto_last: string (nullable = true)\n",
      " |-- client_id_col: string (nullable = true)\n",
      " |-- source_date_col: string (nullable = true)\n",
      " |-- processing_timestamp: timestamp (nullable = true)\n",
      " |-- processing_date: date (nullable = true)\n",
      " |-- load_type: string (nullable = true)\n",
      "\n",
      "+-----------------------+\n",
      "|recipient_fields_status|\n",
      "+-----------------------+\n",
      "|                Applied|\n",
      "|                Applied|\n",
      "|                Applied|\n",
      "|                Applied|\n",
      "|                Applied|\n",
      "|                Applied|\n",
      "|                Applied|\n",
      "|                Applied|\n",
      "|                Applied|\n",
      "|                Applied|\n",
      "|                Applied|\n",
      "|                Applied|\n",
      "|                Applied|\n",
      "|                Applied|\n",
      "|                Applied|\n",
      "|                Applied|\n",
      "|                Applied|\n",
      "|                Applied|\n",
      "|                Applied|\n",
      "|                Applied|\n",
      "+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df3 = spark.read.parquet(\n",
    "    \"s3a://mailshake-analytics/curated/entity=created_leads/client_id=client_2/source_date=2025-12-25/\")\n",
    "\n",
    "# Quick check\n",
    "print(f\"Missing in DF2: {set(df1.columns) - set(df2.columns)}\")\n",
    "print(f\"Missing in DF1: {set(df2.columns) - set(df1.columns)}\")\n",
    "print(f\"Missing in DF3: {set(df3.columns) - set(df1.columns)}\")\n",
    "print(f\"Missing in DF3: {set(df3.columns) - set(df2.columns)}\")\n",
    "print(len(df3.columns))\n",
    "df3.printSchema()\n",
    "df3.select(\"recipient_fields_status\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11af3f44-07ea-43e7-8a57-1c101fd572e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- object: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- actionDate: string (nullable = true)\n",
      " |-- isDuplicate: boolean (nullable = true)\n",
      " |-- recipient_object: string (nullable = true)\n",
      " |-- recipient_id: long (nullable = true)\n",
      " |-- recipient_emailAddress: string (nullable = true)\n",
      " |-- recipient_fullName: string (nullable = true)\n",
      " |-- recipient_created: string (nullable = true)\n",
      " |-- recipient_isPaused: boolean (nullable = true)\n",
      " |-- recipient_contactID: long (nullable = true)\n",
      " |-- recipient_first: string (nullable = true)\n",
      " |-- recipient_last: string (nullable = true)\n",
      " |-- recipient_fields_link: string (nullable = true)\n",
      " |-- recipient_fields_first: string (nullable = true)\n",
      " |-- recipient_fields_position: string (nullable = true)\n",
      " |-- recipient_fields_date applied: string (nullable = true)\n",
      " |-- recipient_fields_account: string (nullable = true)\n",
      " |-- recipient_fields_phoneNumber: string (nullable = true)\n",
      " |-- recipient_fields_facebookUrl: string (nullable = true)\n",
      " |-- recipient_fields_instagramID: string (nullable = true)\n",
      " |-- recipient_fields_linkedInUrl: string (nullable = true)\n",
      " |-- recipient_fields_twitterID: string (nullable = true)\n",
      " |-- campaign_object: string (nullable = true)\n",
      " |-- campaign_id: long (nullable = true)\n",
      " |-- campaign_title: string (nullable = true)\n",
      " |-- campaign_wizardStatus: integer (nullable = true)\n",
      " |-- parent_object: string (nullable = true)\n",
      " |-- parent_id: long (nullable = true)\n",
      " |-- parent_type: string (nullable = true)\n",
      " |-- parent_message_object: string (nullable = true)\n",
      " |-- parent_message_id: long (nullable = true)\n",
      " |-- parent_message_type: string (nullable = true)\n",
      " |-- parent_message_subject: string (nullable = true)\n",
      " |-- parent_message_replyToID: double (nullable = true)\n",
      " |-- client_id_col: string (nullable = true)\n",
      " |-- source_date_col: string (nullable = true)\n",
      " |-- processing_timestamp: timestamp (nullable = true)\n",
      " |-- processing_date: date (nullable = true)\n",
      " |-- load_type: string (nullable = true)\n",
      " |-- recipient_fields_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9813a2-8a42-49ea-b40e-cd6300a701e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
