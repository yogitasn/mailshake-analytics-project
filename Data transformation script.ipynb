{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70fd47d8-e728-49b7-9e18-15474dcb29d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading boto3-1.37.38-py3-none-any.whl (139 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 139 kB 5.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.12.0,>=0.11.0\n",
      "  Downloading s3transfer-0.11.5-py3-none-any.whl (84 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 84 kB 2.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting botocore<1.38.0,>=1.37.38\n",
      "  Downloading botocore-1.37.38-py3-none-any.whl (13.5 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13.5 MB 5.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.38.0,>=1.37.38->boto3) (2.9.0.post0)\n",
      "Collecting urllib3<1.27,>=1.25.4; python_version < \"3.10\"\n",
      "  Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 144 kB 6.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.38->boto3) (1.17.0)\n",
      "Installing collected packages: jmespath, urllib3, botocore, s3transfer, boto3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.3\n",
      "    Uninstalling urllib3-2.2.3:\n",
      "      Successfully uninstalled urllib3-2.2.3\n",
      "Successfully installed boto3-1.37.38 botocore-1.37.38 jmespath-1.0.1 s3transfer-0.11.5 urllib3-1.26.20\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3a4d0b2-c088-4525-aba0-f0f02f500ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded clients: ['client_4', 'client_5', 'client_6']\n",
      "üìÇ activity_open | client_4 | snapshot\n",
      "‚ö†Ô∏è Adding missing column: recipient_fields_status\n",
      "‚ö†Ô∏è Adding missing column: recipient_fields_first\n",
      "2026-01-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 100 records\n",
      "üìÇ activity_open | client_5 | snapshot\n",
      "‚ö†Ô∏è Adding missing column: recipient_fields_status\n",
      "‚ö†Ô∏è Adding missing column: recipient_fields_first\n",
      "2026-01-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 100 records\n",
      "üìÇ activity_open | client_6 | snapshot\n",
      "‚ö†Ô∏è Adding missing column: recipient_fields_status\n",
      "‚ö†Ô∏è Adding missing column: recipient_fields_first\n",
      "2026-01-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 100 records\n",
      "Loaded last_dates from curated:\n",
      "  client_12: 2026-01-04\n",
      "  client_2: 2025-12-25\n",
      "  client_8: 2026-01-05\n",
      "  client_9: 2026-01-05\n",
      "  client_10: 2026-01-04\n",
      "  client_11: 2026-01-04\n",
      "  client_7: 2026-01-05\n",
      "  client_3: 2025-12-25\n",
      "  client_1: 2025-12-25\n",
      "üìÇ activity_reply | client_4 | snapshot\n",
      "‚ö†Ô∏è Adding missing column: recipient_fields_status\n",
      "‚ö†Ô∏è Adding missing column: recipient_fields_first\n",
      "2026-01-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 25 records\n",
      "üìÇ activity_reply | client_5 | snapshot\n",
      "‚ö†Ô∏è Adding missing column: recipient_fields_status\n",
      "‚ö†Ô∏è Adding missing column: recipient_fields_first\n",
      "2026-01-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 25 records\n",
      "üìÇ activity_reply | client_6 | snapshot\n",
      "‚ö†Ô∏è Adding missing column: recipient_fields_status\n",
      "‚ö†Ô∏è Adding missing column: recipient_fields_first\n",
      "2026-01-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 25 records\n",
      "Loaded last_dates from curated:\n",
      "  client_10: 2026-01-04\n",
      "  client_3: 2025-12-25\n",
      "  client_8: 2026-01-05\n",
      "  client_2: 2025-12-25\n",
      "  client_7: 2026-01-05\n",
      "  client_1: 2025-12-25\n",
      "  client_12: 2026-01-04\n",
      "  client_9: 2026-01-05\n",
      "  client_11: 2026-01-04\n",
      "üìÇ activity_sent | client_4 | snapshot\n",
      "‚ö†Ô∏è Adding missing column: recipient_fields_status\n",
      "2026-01-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 25 records\n",
      "üìÇ activity_sent | client_5 | snapshot\n",
      "‚ö†Ô∏è Adding missing column: recipient_fields_status\n",
      "2026-01-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 25 records\n",
      "üìÇ activity_sent | client_6 | snapshot\n",
      "‚ö†Ô∏è Adding missing column: recipient_fields_status\n",
      "2026-01-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 25 records\n",
      "üéâ All datasets processed successfully!\n"
     ]
    }
   ],
   "source": [
    "#new code\n",
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col, max, lit, current_timestamp, current_date, explode_outer\n",
    ")\n",
    "from pyspark.sql.types import NullType, StringType, DoubleType, StructType, ArrayType,LongType\n",
    "from datetime import datetime, timedelta\n",
    "import os, re, json, boto3\n",
    "from typing import Dict\n",
    "from datetime import datetime, date\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIG\n",
    "# ============================================================================\n",
    "RAW_PATH = \"s3a://mailshake-analytics/raw\"\n",
    "CURATED_PATH = \"s3a://mailshake-analytics/curated\"\n",
    "curated_base_path = \"s3a://mailshake-analytics/curated\"\n",
    "raw_base_path = \"s3a://mailshake-analytics/raw\"\n",
    "BUCKET = \"mailshake-analytics\"\n",
    "CLIENTS_KEY = \"config/clients_test.json\"\n",
    "RUN_DATE = datetime.utcnow().strftime(\"%Y-%m-%d\")\n",
    "SINGLE_DATE = None        # None for incremental activities\n",
    "# ============================================================================\n",
    "# SPARK SESSION\n",
    "# ============================================================================\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"MailshakeCampaignCurations\")\n",
    "    .config(\n",
    "        \"spark.driver.extraClassPath\",\n",
    "        \"/opt/spark/jars/hadoop-aws-3.3.4.jar:/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.executor.extraClassPath\",\n",
    "        \"/opt/spark/jars/hadoop-aws-3.3.4.jar:/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "\n",
    "# ============================================================================\n",
    "# S3 CONFIG\n",
    "# ============================================================================\n",
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", os.getenv(\"AWS_ACCESS_KEY_ID\"))\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", os.getenv(\"AWS_SECRET_ACCESS_KEY\"))\n",
    "hadoop_conf.set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "# ============================================================================\n",
    "# CLIENT LOADING\n",
    "# ============================================================================\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "def load_clients() -> Dict[str, Dict[str, str]]:\n",
    "    obj = s3.get_object(Bucket=BUCKET, Key=CLIENTS_KEY)\n",
    "    return json.loads(obj[\"Body\"].read().decode(\"utf-8\")).get(\"clients\", {})\n",
    "\n",
    "clients_dict = load_clients()\n",
    "CLIENT_IDS = list(clients_dict.keys())\n",
    "print(f\"Loaded clients: {CLIENT_IDS}\")\n",
    "\n",
    "# ============================================================================\n",
    "# HELPERS\n",
    "# ============================================================================\n",
    "def sanitize_column_names(df):\n",
    "    for col_name in df.columns:\n",
    "        clean = re.sub(r'[^a-zA-Z0-9_]', '_', col_name)\n",
    "        clean = re.sub(r'_+', '_', clean).lower()\n",
    "        if clean != col_name:\n",
    "            df = df.withColumnRenamed(col_name, clean)\n",
    "    return df\n",
    "\n",
    "def fix_void_columns(df):\n",
    "    for field in df.schema.fields:\n",
    "        if isinstance(field.dataType, NullType):\n",
    "            df = df.withColumn(field.name, col(field.name).cast(StringType()))\n",
    "    return df\n",
    "\n",
    "def ensure_columns_and_reorder(df: DataFrame, column_order: list, column_types: dict = None) -> DataFrame:\n",
    "    column_types = column_types or {}\n",
    "    for col_name in column_order:\n",
    "        if col_name not in df.columns:\n",
    "            dtype = column_types.get(col_name)\n",
    "            if dtype:\n",
    "                df = df.withColumn(col_name, lit(None).cast(dtype))\n",
    "            else:\n",
    "                df = df.withColumn(col_name, lit(None))\n",
    "            print(f\"‚ö†Ô∏è Adding missing column: {col_name}\")\n",
    "    return df.select([col(c) for c in column_order])\n",
    "\n",
    "def flatten_struct_columns(df):\n",
    "    while True:\n",
    "        struct_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, StructType)]\n",
    "        if not struct_cols: break\n",
    "        for col_name in struct_cols:\n",
    "            for nested in df.schema[col_name].dataType.fields:\n",
    "                df = df.withColumn(f\"{col_name}_{nested.name}\", col(f\"{col_name}.{nested.name}\"))\n",
    "            df = df.drop(col_name)\n",
    "\n",
    "    array_struct_cols = [\n",
    "        f.name for f in df.schema.fields\n",
    "        if isinstance(f.dataType, ArrayType) and isinstance(f.dataType.elementType, StructType)\n",
    "    ]\n",
    "    for col_name in array_struct_cols:\n",
    "        df = df.withColumn(col_name, explode_outer(col(col_name)))\n",
    "        for nested in df.schema[col_name].dataType.fields:\n",
    "            df = df.withColumn(f\"{col_name}_{nested.name}\", col(f\"{col_name}.{nested.name}\"))\n",
    "        df = df.drop(col_name)\n",
    "    return df\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_dates_to_process(curated_path, raw_base_path, dataset_name, client_ids, single_date=None):\n",
    "    \"\"\"\n",
    "    Returns dict:\n",
    "    - client_id -> list of incremental event_dates that ACTUALLY exist in raw\n",
    "    - Empty list => snapshot\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    bucket = \"mailshake-analytics\"\n",
    "\n",
    "    # Manual override\n",
    "    if single_date:\n",
    "        return {c: [single_date] for c in client_ids}\n",
    "\n",
    "    # Campaigns never use incremental\n",
    "    if dataset_name.startswith(\"campaign\"):\n",
    "        return {c: [] for c in client_ids}\n",
    "\n",
    "    # --- Read curated to get last processed date ---\n",
    "    try:\n",
    "        existing = spark.read.parquet(curated_path)\n",
    "        last_dates = (\n",
    "            existing.groupBy(\"client_id\")\n",
    "            .agg(max(\"source_date\").alias(\"last_date\"))\n",
    "            .collect()\n",
    "        )\n",
    "        last_map = {r[\"client_id\"]: r[\"last_date\"] for r in last_dates}\n",
    "        print(\"Loaded last_dates from curated:\")\n",
    "        for k, v in last_map.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "    except Exception:\n",
    "        last_map = {}\n",
    "\n",
    "    dates = {}\n",
    "\n",
    "    for client in client_ids:\n",
    "        # No curated ‚Üí snapshot\n",
    "        if client not in last_map:\n",
    "            dates[client] = []\n",
    "            continue\n",
    "\n",
    "        last_date = datetime.strptime(str(last_map[client]), \"%Y-%m-%d\").date()\n",
    "\n",
    "        # List S3 folders for this client & dataset\n",
    "        prefix = f\"raw/client_id={client}/entity={dataset_name}/\"\n",
    "        incremental_dates = []\n",
    "\n",
    "        try:\n",
    "            paginator = s3_client.get_paginator(\"list_objects_v2\")\n",
    "            pages = paginator.paginate(Bucket=bucket, Prefix=prefix, Delimiter='/')\n",
    "\n",
    "            for page in pages:\n",
    "                for cp in page.get(\"CommonPrefixes\", []):\n",
    "                    folder_name = cp.get(\"Prefix\").rstrip('/').split('/')[-1]  # e.g., event_date=2026-01-03\n",
    "                    if folder_name.startswith(\"event_date=\"):\n",
    "                        d_str = folder_name.split(\"=\")[1]\n",
    "                        d_dt = datetime.strptime(d_str, \"%Y-%m-%d\").date()\n",
    "                        print(f\"Found S3 folder: {d_dt}\")\n",
    "                        if d_dt > last_date:\n",
    "                            incremental_dates.append(d_str)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not list raw path {prefix}: {e}\")\n",
    "            incremental_dates = []\n",
    "\n",
    "        dates[client] = sorted(incremental_dates)\n",
    "        print(f\"{client} last_date={last_date}, incremental_dates={dates[client]}\")\n",
    "\n",
    "    return dates\n",
    "\n",
    "\n",
    "def process_dataset(\n",
    "    raw_base_path, curated_base_path, client_ids, dataset_name,\n",
    "    unique_keys, explode_col=None, dates_per_client=None,\n",
    "    desired_columns=None, column_types=None\n",
    "):\n",
    "    entity_path = f\"{curated_base_path}/entity={dataset_name}\"\n",
    "\n",
    "    for client_id in client_ids:\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # Decide snapshot vs incremental (ONCE)\n",
    "        # ------------------------------------------------------------------\n",
    "        curated_client_path = f\"{curated_base_path}/entity={dataset_name}/client_id={client_id}\"\n",
    "\n",
    "        if dataset_name.startswith(\"campaign\"):\n",
    "            snapshot_mode = True\n",
    "        else:\n",
    "            try:\n",
    "                spark.read.parquet(curated_client_path)\n",
    "                snapshot_mode = False   # curated exists ‚Üí incremental\n",
    "            except Exception:\n",
    "                snapshot_mode = True    # first run ‚Üí snapshot\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # Determine paths to process\n",
    "        # ------------------------------------------------------------------\n",
    "        paths_to_process = []\n",
    "\n",
    "        if snapshot_mode:\n",
    "            paths_to_process.append(\"snapshot\")\n",
    "        else:\n",
    "            incremental_dates = (dates_per_client or {}).get(client_id, [])\n",
    "            if not incremental_dates:\n",
    "                print(f\"‚ö†Ô∏è No incremental dates for {dataset_name} | {client_id}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            for d in incremental_dates:\n",
    "                paths_to_process.append(f\"event_date={d}\")\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # Process each path\n",
    "        # ------------------------------------------------------------------\n",
    "        for p in paths_to_process:\n",
    "            input_path = f\"{raw_base_path}/client_id={client_id}/entity={dataset_name}/{p}/\"\n",
    "\n",
    "            try:\n",
    "                print(f\"üìÇ {dataset_name} | {client_id} | {p}\")\n",
    "                df = spark.read.parquet(input_path)\n",
    "\n",
    "                # Flatten & explode\n",
    "                df = flatten_struct_columns(df)\n",
    "                if explode_col:\n",
    "                    explode_col_ = explode_col.replace(\".\", \"_\")\n",
    "                    if explode_col_ in df.columns:\n",
    "                        df = df.withColumn(explode_col_, explode_outer(col(explode_col_)))\n",
    "\n",
    "                # Sanitize & align schema\n",
    "                df = sanitize_column_names(df)\n",
    "                df = ensure_columns_and_reorder(df, desired_columns, column_types)\n",
    "\n",
    "                # ------------------------------------------------------------------\n",
    "                # Source date logic (REAL FIX)\n",
    "                # ------------------------------------------------------------------\n",
    "                if p.startswith(\"event_date=\"):\n",
    "                    source_date_val = p.split(\"=\")[1]   # incremental\n",
    "                else:\n",
    "                    # snapshot ‚Üí derive from data\n",
    "                    if \"actiondate\" in df.columns:\n",
    "                        source_date_val = (\n",
    "                            df.selectExpr(\"date(actiondate) as d\")\n",
    "                              .agg({\"d\": \"max\"})\n",
    "                              .collect()[0][0]\n",
    "                        )\n",
    "                    elif \"created\" in df.columns:\n",
    "                        source_date_val = (\n",
    "                            df.selectExpr(\"date(created) as d\")\n",
    "                              .agg({\"d\": \"max\"})\n",
    "                              .collect()[0][0]\n",
    "                        )\n",
    "                    else:\n",
    "                        source_date_val = RUN_DATE\n",
    "\n",
    "                source_date_val = str(source_date_val)\n",
    "                print(source_date_val)\n",
    "\n",
    "                # ------------------------------------------------------------------\n",
    "                # Metadata\n",
    "                # ------------------------------------------------------------------\n",
    "                df = (\n",
    "                    df.withColumn(\"client_id\", lit(client_id))\n",
    "                      .withColumn(\"source_date\", lit(source_date_val))\n",
    "                      .withColumn(\"client_id_col\", lit(client_id))\n",
    "                      .withColumn(\"source_date_col\", lit(source_date_val))\n",
    "                      .withColumn(\"processing_timestamp\", current_timestamp())\n",
    "                      .withColumn(\"processing_date\", current_date())\n",
    "                      .withColumn(\n",
    "                          \"load_type\",\n",
    "                          lit(\"snapshot\" if snapshot_mode else \"incremental\")\n",
    "                      )\n",
    "                )\n",
    "\n",
    "                # Deduplication\n",
    "                safe_keys = [k.replace(\".\", \"_\") for k in unique_keys]\n",
    "                df = df.dropDuplicates(safe_keys + [\"client_id\", \"source_date\"])\n",
    "                df = fix_void_columns(df)\n",
    "\n",
    "                # ------------------------------------------------------------------\n",
    "                # Write\n",
    "                # ------------------------------------------------------------------\n",
    "                write_mode = \"overwrite\" if snapshot_mode else \"append\"\n",
    "\n",
    "                df.write \\\n",
    "                    .mode(write_mode) \\\n",
    "                    .partitionBy(\"client_id\", \"source_date\") \\\n",
    "                    .parquet(entity_path)\n",
    "\n",
    "                print(f\"‚úÖ Written {df.count()} records\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Skipped {dataset_name} | {client_id} | {p}: {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# RUN\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "# process_dataset(\n",
    "#     RAW_PATH, CURATED_PATH, CLIENT_IDS,\n",
    "#     \"campaign\",\n",
    "#     unique_keys=[\"id\"],  # deduplicate by campaign id\n",
    "#     dates_per_client= None,\n",
    "#     desired_columns=[\n",
    "#         \"object\", \"id\", \"title\", \"created\", \"isarchived\", \"ispaused\", \"wizardstatus\", \"url\",\n",
    "#         \"sender_object\", \"sender_id\", \"sender_emailaddress\", \"sender_fromname\", \"sender_created\",\n",
    "#         \"message_id\", \"message_ispaused\", \"message_object\", \"message_replytoid\", \"message_subject\", \"message_type\"\n",
    "#     ],\n",
    "#     column_types= None\n",
    "# )\n",
    "\n",
    "# dates_per_client = get_dates_to_process(\n",
    "#     curated_path=f\"{CURATED_PATH}/entity=activity_open\",\n",
    "#     raw_base_path=RAW_PATH,\n",
    "#     dataset_name=\"activity_open\",\n",
    "#     client_ids=CLIENT_IDS,\n",
    "#     single_date=None\n",
    "# )\n",
    "\n",
    "# -------------------- activity_open --------------------\n",
    "process_dataset(\n",
    "    RAW_PATH, CURATED_PATH, CLIENT_IDS,\n",
    "    \"activity_open\",\n",
    "    unique_keys=[\"id\", \"recipient.id\", \"campaign.id\"],\n",
    "    dates_per_client=dates_per_client,\n",
    "    desired_columns = [\n",
    "        \"object\", \"id\", \"actiondate\", \"isduplicate\", \"recipient_object\",\n",
    "        \"recipient_id\", \"recipient_emailaddress\", \"recipient_fullname\",\n",
    "        \"recipient_created\", \"recipient_ispaused\", \"recipient_contactid\",\n",
    "        \"recipient_first\", \"recipient_last\", \"recipient_fields_link\",\n",
    "        \"recipient_fields_status\", \"recipient_fields_first\",\n",
    "        \"recipient_fields_position\", \"recipient_fields_date_applied\",\n",
    "        \"recipient_fields_account\", \"recipient_fields_phonenumber\",\n",
    "        \"recipient_fields_facebookurl\", \"recipient_fields_instagramid\",\n",
    "        \"recipient_fields_linkedinurl\", \"recipient_fields_twitterid\",\n",
    "        \"campaign_object\", \"campaign_id\", \"campaign_title\", \"campaign_wizardstatus\",\n",
    "        \"parent_object\", \"parent_id\", \"parent_type\", \"parent_message_object\",\n",
    "        \"parent_message_id\", \"parent_message_type\", \"parent_message_subject\",\n",
    "        \"parent_message_replytoid\"\n",
    "    ],\n",
    "    column_types = {\"recipient_fields_status\": StringType(),\n",
    "                    \"recipient_fields_first\": StringType(),\n",
    "                   \"parent_message_replytoid\": LongType()}\n",
    ")\n",
    "\n",
    "dates_per_client = get_dates_to_process(\n",
    "    curated_path=f\"{CURATED_PATH}/entity=activity_reply\",\n",
    "    raw_base_path=RAW_PATH,\n",
    "    dataset_name=\"activity_reply\",\n",
    "    client_ids=CLIENT_IDS,\n",
    "    single_date=None\n",
    ")\n",
    "\n",
    "# -------------------- activity_reply --------------------\n",
    "process_dataset(\n",
    "    RAW_PATH, CURATED_PATH, CLIENT_IDS,\n",
    "    \"activity_reply\",\n",
    "    unique_keys=[\"id\", \"recipient.id\", \"campaign.id\"],\n",
    "    dates_per_client=dates_per_client,\n",
    "    desired_columns = [\n",
    "        \"object\", \"id\", \"actiondate\", \"type\", \"subject\", \"externalid\",\n",
    "        \"externalrawmessageid\", \"externalconversationid\", \"rawbody\", \"body\", \"plaintextbody\",\n",
    "        \"recipient_object\", \"recipient_id\", \"recipient_emailaddress\", \"recipient_fullname\",\n",
    "        \"recipient_created\", \"recipient_ispaused\", \"recipient_contactid\",\n",
    "        \"recipient_first\", \"recipient_last\", \"recipient_fields_link\",\n",
    "        \"recipient_fields_status\",  \"recipient_fields_first\", \"recipient_fields_position\", \"recipient_fields_date_applied\",\n",
    "        \"recipient_fields_account\", \"recipient_fields_phonenumber\", \"recipient_fields_facebookurl\",\n",
    "        \"recipient_fields_instagramid\", \"recipient_fields_linkedinurl\", \"recipient_fields_twitterid\",\n",
    "        \"campaign_object\", \"campaign_id\", \"campaign_title\", \"campaign_wizardstatus\",\n",
    "        \"parent_object\", \"parent_id\", \"parent_type\", \"parent_message_object\", \"parent_message_id\",\n",
    "        \"parent_message_type\", \"parent_message_subject\", \"parent_message_replytoid\",\n",
    "        \"from_object\", \"from_address\", \"from_fullname\", \"from_first\", \"from_last\"\n",
    "    ],\n",
    "    column_types = {\"recipient_fields_status\": StringType(), \n",
    "                    \"recipient_fields_first\": StringType(),\n",
    "                   \"parent_message_replytoid\": LongType()}\n",
    ")\n",
    "\n",
    "dates_per_client = get_dates_to_process(\n",
    "    curated_path=f\"{CURATED_PATH}/entity=activity_sent\",\n",
    "    raw_base_path=RAW_PATH,\n",
    "    dataset_name=\"activity_sent\",\n",
    "    client_ids=CLIENT_IDS,\n",
    "    single_date=None\n",
    ")\n",
    "\n",
    "# -------------------- activity_sent --------------------\n",
    "process_dataset(\n",
    "    RAW_PATH, CURATED_PATH, CLIENT_IDS,\n",
    "    \"activity_sent\",\n",
    "    unique_keys=[\"id\", \"recipient.id\", \"campaign.id\"],\n",
    "    explode_col=\"to\",\n",
    "    dates_per_client=dates_per_client,\n",
    "    desired_columns = [\n",
    "        \"object\", \"id\", \"actiondate\", \"type\", \"excludebody\",\n",
    "        \"to_address\", \"to_first\", \"to_fullname\", \"to_last\", \"to_object\",\n",
    "        \"subject\", \"externalid\", \"externalrawmessageid\", \"externalconversationid\", \"rawbody\", \"body\", \"plaintextbody\",\n",
    "        \"recipient_object\", \"recipient_id\", \"recipient_emailaddress\", \"recipient_fullname\", \"recipient_created\", \"recipient_ispaused\",\n",
    "        \"recipient_first\", \"recipient_last\",\n",
    "        \"recipient_fields_account\", \"recipient_fields_phonenumber\",\n",
    "        \"recipient_fields_facebookurl\", \"recipient_fields_instagramid\",\n",
    "        \"recipient_fields_linkedinurl\", \"recipient_fields_twitterid\",\n",
    "        \"recipient_fields_link\", \"recipient_fields_position\", \"recipient_fields_date_applied\", \"recipient_fields_status\",\n",
    "        \"campaign_object\", \"campaign_id\", \"campaign_title\", \"campaign_wizardstatus\",\n",
    "        \"message_object\", \"message_id\", \"message_type\", \"message_subject\", \"message_replytoid\",\n",
    "        \"from_object\", \"from_address\", \"from_fullname\", \"from_first\", \"from_last\"\n",
    "    ],\n",
    "    column_types={\"recipient_fields_status\": StringType(),\n",
    "                 \"message_replytoid\": DoubleType()}\n",
    ")\n",
    "\n",
    "# -------------------- created_leads --------------------\n",
    "# process_dataset(\n",
    "#     RAW_PATH, CURATED_PATH, CLIENT_IDS,\n",
    "#     \"created_leads\",\n",
    "#     unique_keys=[\"id\", \"recipient.id\", \"campaign.id\"],\n",
    "#     dates_per_client=dates_per_client,\n",
    "#     desired_columns = [\n",
    "#         \"object\", \"id\", \"created\", \"openeddate\", \"laststatuschangedate\", \"annotation\", \"status\",\n",
    "#         \"recipient_object\", \"recipient_id\", \"recipient_emailaddress\", \"recipient_fullname\", \"recipient_created\", \"recipient_ispaused\",\n",
    "#         \"recipient_contactid\", \"recipient_first\", \"recipient_last\",\n",
    "#         \"recipient_fields_link\", \"recipient_fields_first\", \"recipient_fields_status\",\n",
    "#         \"recipient_fields_position\", \"recipient_fields_date_applied\", \"recipient_fields_account\",\n",
    "#         \"recipient_fields_phonenumber\", \"recipient_fields_facebookurl\", \"recipient_fields_instagramid\",\n",
    "#         \"recipient_fields_linkedinurl\", \"recipient_fields_twitterid\",\n",
    "#         \"campaign_object\", \"campaign_id\", \"campaign_title\", \"campaign_wizardstatus\",\n",
    "#         \"assignedto_object\", \"assignedto_id\", \"assignedto_emailaddress\", \"assignedto_fullname\",\n",
    "#         \"assignedto_first\", \"assignedto_last\"\n",
    "#     ],\n",
    "#     column_types={\n",
    "#         \"recipient_fields_status\": StringType(),\n",
    "#         \"recipient_fields_first\": StringType(),\n",
    "#         \"assignedto_object\": StringType(),\n",
    "#         \"assignedto_id\": DoubleType(),\n",
    "#         \"assignedto_emailaddress\": StringType(),\n",
    "#         \"assignedto_fullname\": StringType(),\n",
    "#         \"assignedto_first\": StringType(),\n",
    "#         \"assignedto_last\": StringType()\n",
    "#     }\n",
    "# )\n",
    "\n",
    "spark.stop()\n",
    "print(\"üéâ All datasets processed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef2e17c-4021-4ab2-bc39-be43103b6f81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
