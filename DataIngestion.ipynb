{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31ff4bfb-6012-4c21-9ad8-fa190135011b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12.4 MB 28.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2025.2)\n",
      "Collecting tzdata>=2022.1\n",
      "  Downloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 348 kB 7.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.20.3; python_version < \"3.10\"\n",
      "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17.3 MB 5.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Installing collected packages: tzdata, numpy, pandas\n",
      "Successfully installed numpy-1.24.4 pandas-2.0.3 tzdata-2025.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1285345f-ce9c-4e9f-8eb6-4161fcf849c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading boto3-1.37.38-py3-none-any.whl (139 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 139 kB 7.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting botocore<1.38.0,>=1.37.38\n",
      "  Downloading botocore-1.37.38-py3-none-any.whl (13.5 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13.5 MB 6.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3transfer<0.12.0,>=0.11.0\n",
      "  Downloading s3transfer-0.11.5-py3-none-any.whl (84 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 84 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting urllib3<1.27,>=1.25.4; python_version < \"3.10\"\n",
      "  Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 144 kB 49.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.38.0,>=1.37.38->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.38->boto3) (1.17.0)\n",
      "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.3\n",
      "    Uninstalling urllib3-2.2.3:\n",
      "      Successfully uninstalled urllib3-2.2.3\n",
      "Successfully installed boto3-1.37.38 botocore-1.37.38 jmespath-1.0.1 s3transfer-0.11.5 urllib3-1.26.20\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dea47a8d-3089-4987-80c3-c2153efb337a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-17.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.0 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40.0 MB 3.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from pyarrow) (1.24.4)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-17.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c2faa7d-1535-4a36-b80b-814962644b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.8/dist-packages (1.37.38)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from boto3) (0.11.5)\n",
      "Requirement already satisfied: botocore<1.38.0,>=1.37.38 in /usr/local/lib/python3.8/dist-packages (from boto3) (1.37.38)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from botocore<1.38.0,>=1.37.38->boto3) (1.26.20)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.38.0,>=1.37.38->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.38->boto3) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adac5485-df6c-4803-8e5b-cd8faae207de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-17.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.0 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40.0 MB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.16.6\n",
      "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17.3 MB 56 kB/s  eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: numpy, pyarrow\n",
      "Successfully installed numpy-1.24.4 pyarrow-17.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11c1797f-3a46-4a3c-9764-54b8feae9e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.20.3; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff2015bb-f99c-4902-b765-91abf099f2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ STARTING CLIENT client_10\n",
      "üü¶ SNAPSHOT ‚Üí campaign\n",
      "‚û° activity_open | first_run=True\n",
      "üü¶ SNAPSHOT ‚Üí activity_open\n",
      "üîÅ Watermark updated ‚Üí 2025-09-18T05:45:23+0000\n",
      "‚û° activity_reply | first_run=True\n",
      "üü¶ SNAPSHOT ‚Üí activity_reply\n",
      "üîÅ Watermark updated ‚Üí 2025-09-09T16:37:23+0000\n",
      "‚û° activity_sent | first_run=True\n",
      "üü¶ SNAPSHOT ‚Üí activity_sent\n",
      "üîÅ Watermark updated ‚Üí 2025-09-11T12:01:00+0000\n",
      "‚û° created_leads | first_run=True\n",
      "üü¶ SNAPSHOT ‚Üí created_leads\n",
      "üîÅ Watermark updated ‚Üí 2025-09-09T16:49:27+0000\n",
      "‚úÖ CLIENT client_10 COMPLETED\n",
      "\n",
      "üöÄ STARTING CLIENT client_11\n",
      "üü¶ SNAPSHOT ‚Üí campaign\n",
      "‚û° activity_open | first_run=True\n",
      "üü¶ SNAPSHOT ‚Üí activity_open\n",
      "üîÅ Watermark updated ‚Üí 2026-01-03T02:23:14+0000\n",
      "‚û° activity_reply | first_run=True\n",
      "üü¶ SNAPSHOT ‚Üí activity_reply\n",
      "üîÅ Watermark updated ‚Üí 2025-12-30T15:43:44+0000\n",
      "‚û° activity_sent | first_run=True\n",
      "üü¶ SNAPSHOT ‚Üí activity_sent\n",
      "üîÅ Watermark updated ‚Üí 2026-01-01T15:00:06+0000\n",
      "‚û° created_leads | first_run=True\n",
      "üü¶ SNAPSHOT ‚Üí created_leads\n",
      "üîÅ Watermark updated ‚Üí 2025-12-30T15:48:00+0000\n",
      "‚úÖ CLIENT client_11 COMPLETED\n",
      "\n",
      "üöÄ STARTING CLIENT client_12\n",
      "üü¶ SNAPSHOT ‚Üí campaign\n",
      "‚û° activity_open | first_run=True\n",
      "üü¶ SNAPSHOT ‚Üí activity_open\n",
      "üîÅ Watermark updated ‚Üí 2026-01-03T10:52:31+0000\n",
      "‚û° activity_reply | first_run=True\n",
      "üü¶ SNAPSHOT ‚Üí activity_reply\n",
      "üîÅ Watermark updated ‚Üí 2025-12-29T02:04:01+0000\n",
      "‚û° activity_sent | first_run=True\n",
      "üü¶ SNAPSHOT ‚Üí activity_sent\n",
      "üîÅ Watermark updated ‚Üí 2026-01-01T14:24:01+0000\n",
      "‚û° created_leads | first_run=True\n",
      "üü¶ SNAPSHOT ‚Üí created_leads\n",
      "üîÅ Watermark updated ‚Üí 2025-12-29T02:12:09+0000\n",
      "‚úÖ CLIENT client_12 COMPLETED\n",
      "\n",
      "üèÅ JOB FINISHED\n"
     ]
    }
   ],
   "source": [
    "#incremental code update\n",
    "\n",
    "#new code\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any\n",
    "import boto3\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================================\n",
    "# GLOBALS\n",
    "# ============================================================================\n",
    "BASE = \"https://api.mailshake.com/2017-04-01\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "BUCKET = \"mailshake-analytics\"\n",
    "RAW_PREFIX = \"raw\"\n",
    "WATERMARK_PREFIX = \"metadata/watermark\"\n",
    "CLIENTS_KEY = \"config/clients_test2.json\"\n",
    "s3 = boto3.client(\"s3\")\n",
    "RUN_DATE = datetime.utcnow().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# ============================================================================\n",
    "# CLIENT CONFIG\n",
    "# ============================================================================\n",
    "def load_clients() -> Dict[str, Dict[str, str]]:\n",
    "    obj = s3.get_object(Bucket=BUCKET, Key=CLIENTS_KEY)\n",
    "    return json.loads(obj[\"Body\"].read().decode(\"utf-8\")).get(\"clients\", {})\n",
    "\n",
    "# ============================================================================\n",
    "# SAFE POST\n",
    "# ============================================================================\n",
    "def safe_post(url: str, payload: Dict[str, Any], auth: HTTPBasicAuth) -> requests.Response:\n",
    "    resp = requests.post(url, json=payload, headers=HEADERS, auth=auth, timeout=30)\n",
    "    if resp.status_code == 429:\n",
    "        print(\"üö´ API RATE LIMIT HIT ‚Äî STOPPING JOB\")\n",
    "        raise SystemExit(1)\n",
    "    resp.raise_for_status()\n",
    "    return resp\n",
    "\n",
    "# ============================================================================\n",
    "# WATERMARK HELPERS\n",
    "# ============================================================================\n",
    "def read_watermarks(client_id: str) -> dict:\n",
    "    key = f\"{WATERMARK_PREFIX}/watermark_{client_id}.json\"\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=BUCKET, Key=key)\n",
    "        return json.loads(obj[\"Body\"].read().decode(\"utf-8\"))\n",
    "    except s3.exceptions.NoSuchKey:\n",
    "        return {}\n",
    "\n",
    "def update_watermarks(client_id: str, new_data: dict):\n",
    "    key = f\"{WATERMARK_PREFIX}/watermark_{client_id}.json\"\n",
    "    current = read_watermarks(client_id)\n",
    "    current.update(new_data)\n",
    "    s3.put_object(Bucket=BUCKET, Key=key, Body=json.dumps(current, indent=2).encode(\"utf-8\"))\n",
    "\n",
    "def get_watermark(client_id: str, entity: str) -> str:\n",
    "    watermarks = read_watermarks(client_id)\n",
    "    return watermarks.get(entity, \"1970-01-01T00:00:00Z\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE PARQUET (SNAPSHOT OR INCREMENTAL)\n",
    "# ============================================================================\n",
    "# ============================================================================\n",
    "# SAVE PARQUET (SNAPSHOT OR INCREMENTAL) ‚Äî FULL SPARK SAFE WITH WATERMARK\n",
    "# ============================================================================\n",
    "def save_snapshot_or_incremental(\n",
    "    data: list, client_id: str, entity: str, ts_col: str, first_run: bool\n",
    "):\n",
    "    if not data:\n",
    "        print(f\"‚ö†Ô∏è No data for {entity}\")\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Convert timestamp column if exists\n",
    "    if ts_col in df.columns:\n",
    "        df[ts_col] = pd.to_datetime(df[ts_col], errors=\"coerce\", utc=True)\n",
    "        df = df.dropna(subset=[ts_col])\n",
    "\n",
    "        if df.empty:\n",
    "            print(f\"‚ö†Ô∏è No valid timestamps for {entity}\")\n",
    "            return None\n",
    "\n",
    "        # Spark-safe ISO format\n",
    "        df[ts_col] = df[ts_col].dt.strftime(\"%Y-%m-%dT%H:%M:%S%z\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è {ts_col} missing ‚Äî saving anyway\")\n",
    "\n",
    "    batch_ts = datetime.utcnow().strftime(\"%Y%m%dT%H%M%S\")\n",
    "\n",
    "    # -------------------------\n",
    "    # FIRST RUN ‚Üí SNAPSHOT\n",
    "    # -------------------------\n",
    "    if first_run:\n",
    "        file = f\"/tmp/{client_id}_{entity}_snapshot.parquet\"\n",
    "        key = f\"{RAW_PREFIX}/client_id={client_id}/entity={entity}/snapshot/{entity}.parquet\"\n",
    "        df.to_parquet(file, index=False)\n",
    "        s3.upload_file(file, BUCKET, key)\n",
    "        print(f\"üü¶ SNAPSHOT ‚Üí {entity}\")\n",
    "        return df[ts_col].max() if ts_col in df.columns else None\n",
    "\n",
    "    # -------------------------\n",
    "    # FILTER BY SAVED WATERMARK\n",
    "    # -------------------------\n",
    "    current_wm = get_watermark(client_id, entity)\n",
    "    if ts_col in df.columns and current_wm:\n",
    "        current_wm_dt = pd.to_datetime(current_wm, utc=True)\n",
    "        df = df[pd.to_datetime(df[ts_col], utc=True) > current_wm_dt]\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"‚ö†Ô∏è No new records for {entity} after watermark {current_wm}\")\n",
    "        return None\n",
    "\n",
    "    # -------------------------\n",
    "    # CREATED_LEADS ‚Üí run_date partition\n",
    "    # -------------------------\n",
    "    if entity == \"created_leads\":\n",
    "        new_wm = df[ts_col].max() if ts_col in df.columns else batch_ts\n",
    "        wm_suffix = new_wm.replace(\":\", \"\").replace(\"-\", \"\")\n",
    "        file = f\"/tmp/{client_id}_{entity}_{wm_suffix}.parquet\"\n",
    "        key = f\"{RAW_PREFIX}/client_id={client_id}/entity={entity}/run_date={RUN_DATE}/{entity}_{wm_suffix}.parquet\"\n",
    "        df.to_parquet(file, index=False)\n",
    "        s3.upload_file(file, BUCKET, key)\n",
    "        print(f\"üü¢ INCREMENTAL ‚Üí {entity} ({len(df)})\")\n",
    "        return new_wm\n",
    "\n",
    "    # -------------------------\n",
    "    # ACTIVITIES ‚Üí event_date partition\n",
    "    # -------------------------\n",
    "    if ts_col in df.columns:\n",
    "        df[\"event_date\"] = pd.to_datetime(df[ts_col], utc=True).dt.strftime(\"%Y-%m-%d\")\n",
    "        new_wm = df[ts_col].max()\n",
    "    else:\n",
    "        df[\"event_date\"] = RUN_DATE\n",
    "        new_wm = batch_ts\n",
    "\n",
    "    for d in df[\"event_date\"].unique():\n",
    "        part = df[df[\"event_date\"] == d].copy()\n",
    "        part.drop(columns=[\"event_date\"], inplace=True)\n",
    "        wm_suffix = new_wm.replace(\":\", \"\").replace(\"-\", \"\")\n",
    "        file = f\"/tmp/{client_id}_{entity}_{d}_{wm_suffix}.parquet\"\n",
    "        key = f\"{RAW_PREFIX}/client_id={client_id}/entity={entity}/event_date={d}/{entity}_{wm_suffix}.parquet\"\n",
    "        part.to_parquet(file, index=False)\n",
    "        s3.upload_file(file, BUCKET, key)\n",
    "        print(f\"üü¢ INCREMENTAL ‚Üí {entity} ({len(part)})\")\n",
    "\n",
    "    return new_wm\n",
    "\n",
    "# ============================================================================\n",
    "# FETCH FUNCTIONS\n",
    "# ============================================================================\n",
    "def fetch_campaigns(team_id: str, auth: HTTPBasicAuth) -> List[Dict[str, Any]]:\n",
    "    resp = safe_post(f\"{BASE}/campaigns/list\", {\"teamID\": team_id}, auth)\n",
    "    return resp.json().get(\"results\", []) or []\n",
    "\n",
    "def fetch_activity(team_id: str, api_name: str, since_ts: str, auth: HTTPBasicAuth) -> List[Dict[str, Any]]:\n",
    "    resp = safe_post(f\"{BASE}/activity/{api_name}\", {\"teamID\": team_id, \"since\": since_ts}, auth)\n",
    "    return resp.json().get(\"results\", []) or []\n",
    "\n",
    "# ============================================================================\n",
    "# RUN SINGLE CLIENT\n",
    "# ============================================================================\n",
    "def run_client(client: Dict[str, str]):\n",
    "    client_id, team_id, auth = client[\"client_id\"], client[\"team_id\"], HTTPBasicAuth(client[\"api_token\"], \"\")\n",
    "    print(f\"\\nüöÄ STARTING CLIENT {client_id}\")\n",
    "\n",
    "    # -------- CAMPAIGNS (FULL LOAD) --------\n",
    "    campaigns = fetch_campaigns(team_id, auth)\n",
    "    if campaigns:\n",
    "        wm = save_snapshot_or_incremental(campaigns, client_id, \"campaign\", \"created\", first_run=True)\n",
    "        if wm:\n",
    "            update_watermarks(client_id, {\"campaign\": wm})\n",
    "\n",
    "    # -------- ACTIVITIES / CREATED_LEADS (INCREMENTAL) --------\n",
    "    activity_map = {\n",
    "        \"activity_open\": (\"opens\", \"actionDate\"),\n",
    "        \"activity_reply\": (\"replies\", \"actionDate\"),\n",
    "        \"activity_sent\": (\"sent\", \"actionDate\"),\n",
    "        \"created_leads\": (\"created-leads\", \"created\")\n",
    "    }\n",
    "\n",
    "    watermarks = read_watermarks(client_id)\n",
    "\n",
    "    for entity, (api, ts_col) in activity_map.items():\n",
    "        watermark = watermarks.get(entity, \"1970-01-01T00:00:00Z\")\n",
    "        first_run = entity not in watermarks\n",
    "        print(f\"‚û° {entity} | first_run={first_run}\")\n",
    "\n",
    "        data = fetch_activity(team_id, api, watermark, auth)\n",
    "        if not data:\n",
    "            print(f\"‚ö†Ô∏è No new records for {entity}\")\n",
    "            continue\n",
    "\n",
    "        new_wm = save_snapshot_or_incremental(data, client_id, entity, ts_col, first_run)\n",
    "\n",
    "        if new_wm:\n",
    "            old_wm = watermarks.get(entity)\n",
    "            if old_wm is None or new_wm > old_wm:\n",
    "                update_watermarks(client_id, {entity: new_wm})\n",
    "                watermarks[entity] = new_wm\n",
    "                print(f\"üîÅ Watermark updated ‚Üí {new_wm}\")\n",
    "\n",
    "    print(f\"‚úÖ CLIENT {client_id} COMPLETED\")\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN LOOP\n",
    "# ============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    clients = load_clients()\n",
    "    for client_id, client_cfg in clients.items():\n",
    "        try:\n",
    "            run_client({\n",
    "                \"client_id\": client_id,\n",
    "                \"team_id\": client_cfg[\"team_id\"],\n",
    "                \"api_token\": client_cfg[\"api_token\"]\n",
    "            })\n",
    "        except SystemExit:\n",
    "            print(\"üö´ API LIMIT HIT ‚Äî STOPPING ALL CLIENTS\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Client {client_id} failed: {e}\")\n",
    "\n",
    "    print(\"\\nüèÅ JOB FINISHED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d73eecc-a74e-4ee5-a93c-609c9558e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SPARK SESSION\n",
    "# ============================================================================\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col, max, lit, current_timestamp, current_date, explode_outer\n",
    ")\n",
    "from pyspark.sql.types import NullType, StringType, StructType, ArrayType\n",
    "from datetime import datetime, timedelta\n",
    "import os, json\n",
    "import re\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"MailshakeCampaignCurations\")\n",
    "    .config(\n",
    "        \"spark.driver.extraClassPath\",\n",
    "        \"/opt/spark/jars/hadoop-aws-3.3.4.jar:/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.executor.extraClassPath\",\n",
    "        \"/opt/spark/jars/hadoop-aws-3.3.4.jar:/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "\n",
    "# ============================================================================\n",
    "# S3 CONFIG\n",
    "# ============================================================================\n",
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", os.getenv(\"AWS_ACCESS_KEY_ID\"))\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", os.getenv(\"AWS_SECRET_ACCESS_KEY\"))\n",
    "hadoop_conf.set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "df = spark.read.parquet(\"s3a://mailshake-analytics/raw/client_id=client_10/entity=activity_open/snapshot/activity_open.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9340fb7-2dfe-43fc-9983-e0da8e19db8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 97\n",
      "+------+-----------+------------------------+-----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "|object|id         |actionDate              |isDuplicate|recipient                                                                                                                                                                                                                                                                                        |campaign                                                   |parent                                                                                                     |\n",
      "+------+-----------+------------------------+-----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "|open  |33970536004|2025-09-18T05:45:23+0000|true       |{387795114, 2025-08-18T10:59:14.987Z, jmalanche@oxalis.io, {Oxalis, 8/18/2025 0:00, , , https://jobs.gusto.com/postings/oxalis-solutions-llc-data-platform-engineer-33288599-78ea-4f43-9a6d-97d9a0f79c0e, , , Data Platform Engineer, }, , , 652061408, false, , recipient}                      |{1451417, campaign, 2025_08_Ninad_Recruiter_Reachout, NULL}|{80425442500, {3976844, message, 3976840, , follow-up}, sent-message, campaign-message}                    |\n",
      "|open  |33930980104|2025-09-12T02:16:45+0000|false      |{389605003, 2025-09-05T11:27:21.769Z, ashwini.k@venturesoft.ai, {VentureSoft, 9/5/2025 0:00, , , https://vstglobal.zohorecruit.com/jobs/Careers/797507000000530480/Data-Scientist?source=CareerSite, , , Data Scientist, }, , , 655250564, false, , recipient}                                   |{1451417, campaign, 2025_08_Ninad_Recruiter_Reachout, NULL}|{80667604600, {3976840, message, NULL, Experienced Data Engineer, initial}, sent-message, campaign-message}|\n",
      "|open  |33930979904|2025-09-12T02:16:45+0000|true       |{389605003, 2025-09-05T11:27:21.769Z, ashwini.k@venturesoft.ai, {VentureSoft, 9/5/2025 0:00, , , https://vstglobal.zohorecruit.com/jobs/Careers/797507000000530480/Data-Scientist?source=CareerSite, , , Data Scientist, }, , , 655250564, false, , recipient}                                   |{1451417, campaign, 2025_08_Ninad_Recruiter_Reachout, NULL}|{80729715300, {3976843, message, 3976840, , follow-up}, sent-message, campaign-message}                    |\n",
      "|open  |33925395004|2025-09-11T12:19:24+0000|true       |{389605005, 2025-09-05T11:27:21.758Z, anima.mund@cba.com.au, {Commonwealth Bank, 9/5/2025 0:00, , , https://cba.wd3.myworkdayjobs.com/CommBank_Careers/job/Bangalore---Manyata-Tech-Park-Road/Data-Scientist_REQ245657?source=LinkedIn, , , Data Scientist, }, , , 655250558, false, , recipient}|{1451417, campaign, 2025_08_Ninad_Recruiter_Reachout, NULL}|{80668382400, {3976840, message, NULL, Experienced Data Engineer, initial}, sent-message, campaign-message}|\n",
      "|open  |33925393804|2025-09-11T12:19:16+0000|false      |{389605005, 2025-09-05T11:27:21.758Z, anima.mund@cba.com.au, {Commonwealth Bank, 9/5/2025 0:00, , , https://cba.wd3.myworkdayjobs.com/CommBank_Careers/job/Bangalore---Manyata-Tech-Park-Road/Data-Scientist_REQ245657?source=LinkedIn, , , Data Scientist, }, , , 655250558, false, , recipient}|{1451417, campaign, 2025_08_Ninad_Recruiter_Reachout, NULL}|{80784025800, {3976844, message, 3976840, , follow-up}, sent-message, campaign-message}                    |\n",
      "|open  |33925393704|2025-09-11T12:19:15+0000|true       |{389605005, 2025-09-05T11:27:21.758Z, anima.mund@cba.com.au, {Commonwealth Bank, 9/5/2025 0:00, , , https://cba.wd3.myworkdayjobs.com/CommBank_Careers/job/Bangalore---Manyata-Tech-Park-Road/Data-Scientist_REQ245657?source=LinkedIn, , , Data Scientist, }, , , 655250558, false, , recipient}|{1451417, campaign, 2025_08_Ninad_Recruiter_Reachout, NULL}|{80730076000, {3976843, message, 3976840, , follow-up}, sent-message, campaign-message}                    |\n",
      "|open  |33917148904|2025-09-10T13:52:18+0000|false      |{389605001, 2025-09-05T11:27:21.769Z, shivani.ushinkar@vanderlande.com, {Vanderlande, 9/5/2025 0:00, , , https://vanderlande.wd3.myworkdayjobs.com/careers/job/Pune-I/Data-Engineer_JR33853?source=LinkedIn, , , Data Engineer, }, , , 655250562, false, , recipient}                            |{1451417, campaign, 2025_08_Ninad_Recruiter_Reachout, NULL}|{80758479200, {3976844, message, 3976840, , follow-up}, sent-message, campaign-message}                    |\n",
      "|open  |33917148804|2025-09-10T13:52:18+0000|true       |{389605001, 2025-09-05T11:27:21.769Z, shivani.ushinkar@vanderlande.com, {Vanderlande, 9/5/2025 0:00, , , https://vanderlande.wd3.myworkdayjobs.com/careers/job/Pune-I/Data-Engineer_JR33853?source=LinkedIn, , , Data Engineer, }, , , 655250562, false, , recipient}                            |{1451417, campaign, 2025_08_Ninad_Recruiter_Reachout, NULL}|{80708821300, {3976843, message, 3976840, , follow-up}, sent-message, campaign-message}                    |\n",
      "|open  |33917148704|2025-09-10T13:52:18+0000|true       |{389605001, 2025-09-05T11:27:21.769Z, shivani.ushinkar@vanderlande.com, {Vanderlande, 9/5/2025 0:00, , , https://vanderlande.wd3.myworkdayjobs.com/careers/job/Pune-I/Data-Engineer_JR33853?source=LinkedIn, , , Data Engineer, }, , , 655250562, false, , recipient}                            |{1451417, campaign, 2025_08_Ninad_Recruiter_Reachout, NULL}|{80666841400, {3976840, message, NULL, Experienced Data Engineer, initial}, sent-message, campaign-message}|\n",
      "|open  |33917007504|2025-09-10T13:37:28+0000|true       |{389605002, 2025-09-05T11:27:21.769Z, mustaq.ansari@schaeffler.com, {Schaeffler, 9/5/2025 0:00, , , https://jobs.schaeffler.com/job/Pune-Data-Eng-DAP-412106/1228463001/?locale=en_US, , , Data Eng - DAP, }, , , 655250563, false, , recipient}                                                 |{1451417, campaign, 2025_08_Ninad_Recruiter_Reachout, NULL}|{80758892000, {3976844, message, 3976840, , follow-up}, sent-message, campaign-message}                    |\n",
      "|open  |33916986704|2025-09-10T13:36:10+0000|false      |{389605002, 2025-09-05T11:27:21.769Z, mustaq.ansari@schaeffler.com, {Schaeffler, 9/5/2025 0:00, , , https://jobs.schaeffler.com/job/Pune-Data-Eng-DAP-412106/1228463001/?locale=en_US, , , Data Eng - DAP, }, , , 655250563, false, , recipient}                                                 |{1451417, campaign, 2025_08_Ninad_Recruiter_Reachout, NULL}|{80758892000, {3976844, message, 3976840, , follow-up}, sent-message, campaign-message}                    |\n",
      "|open  |33910061304|2025-09-09T16:37:22+0000|false      |{389605003, 2025-09-05T11:27:21.769Z, ashwini.k@venturesoft.ai, {VentureSoft, 9/5/2025 0:00, , , https://vstglobal.zohorecruit.com/jobs/Careers/797507000000530480/Data-Scientist?source=CareerSite, , , Data Scientist, }, , , 655250564, false, , recipient}                                   |{1451417, campaign, 2025_08_Ninad_Recruiter_Reachout, NULL}|{80729715300, {3976843, message, 3976840, , follow-up}, sent-message, campaign-message}                    |\n",
      "|open  |33907417504|2025-09-09T12:53:42+0000|false      |{389231710, 2025-09-02T11:34:09.622Z, archana.singh@arista.com, {Arista, 9/2/2025 0:00, , , https://jobs.smartrecruiters.com/AristaNetworks/744000069274379-data-scientist, , , Data Scientist, }, , , 654555285, false, , recipient}                                                            |{1451417, campaign, 2025_08_Ninad_Recruiter_Reachout, NULL}|{80730893900, {3976844, message, 3976840, , follow-up}, sent-message, campaign-message}                    |\n",
      "|open  |33907406004|2025-09-09T12:52:08+0000|false      |{389231709, 2025-09-02T11:34:09.631Z, sudha.natarajan@trellix.com, {Trellix, 9/2/2025 0:00, , , https://trellix.wd1.myworkdayjobs.com/en-US/EnterpriseCareers/job/India-Bangalore/Senior-Data-Scientist_JR0035685, , , Senior Data Scientist, }, , , 654555295, false, , recipient}              |{1451417, campaign, 2025_08_Ninad_Recruiter_Reachout, NULL}|{80665060000, {3976843, message, 3976840, , follow-up}, sent-message, campaign-message}                    |\n",
      "|open  |33907405904|2025-09-09T12:52:08+0000|false      |{389231709, 2025-09-02T11:34:09.631Z, sudha.natarajan@trellix.com, {Trellix, 9/2/2025 0:00, , , https://trellix.wd1.myworkdayjobs.com/en-US/EnterpriseCareers/job/India-Bangalore/Senior-Data-Scientist_JR0035685, , , Senior Data Scientist, }, , , 654555295, false, , recipient}              |{1451417, campaign, 2025_08_Ninad_Recruiter_Reachout, NULL}|{80730446100, {3976844, message, 3976840, , follow-up}, sent-message, campaign-message}                    |\n",
      "|open  |33907111404|2025-09-09T12:22:36+0000|false      |{389605005, 2025-09-05T11:27:21.758Z, anima.mund@cba.com.au, {Commonwealth Bank, 9/5/2025 0:00, , , https://cba.wd3.myworkdayjobs.com/CommBank_Careers/job/Bangalore---Manyata-Tech-Park-Road/Data-Scientist_REQ245657?source=LinkedIn, , , Data Scientist, }, , , 655250558, false, , recipient}|{1451417, campaign, 2025_08_Ninad_Recruiter_Reachout, NULL}|{80730076000, {3976843, message, 3976840, , follow-up}, sent-message, campaign-message}                    |\n",
      "|open  |33907111304|2025-09-09T12:22:36+0000|true       |{389605005, 2025-09-05T11:27:21.758Z, anima.mund@cba.com.au, {Commonwealth Bank, 9/5/2025 0:00, , , https://cba.wd3.myworkdayjobs.com/CommBank_Careers/job/Bangalore---Manyata-Tech-Park-Road/Data-Scientist_REQ245657?source=LinkedIn, , , Data Scientist, }, , , 655250558, false, , recipient}|{1451417, campaign, 2025_08_Ninad_Recruiter_Reachout, NULL}|{80668382400, {3976840, message, NULL, Experienced Data Engineer, initial}, sent-message, campaign-message}|\n",
      "|open  |33906009904|2025-09-09T09:49:53+0000|true       |{389605000, 2025-09-05T11:27:21.769Z, mohini.halingale@ncs.co, {NCS Group, 9/5/2025 0:00, , , https://careers.in.ncs-i.com/ncsi/jobview/senior-data-engineer-pune-maharashtra-india-2025082010193956?source=linkedin, , , Senior Data Engineer, }, , , 655250561, false, , recipient}            |{1451417, campaign, 2025_08_Ninad_Recruiter_Reachout, NULL}|{80708321700, {3976843, message, 3976840, , follow-up}, sent-message, campaign-message}                    |\n",
      "|open  |33905990104|2025-09-09T09:45:37+0000|true       |{389605000, 2025-09-05T11:27:21.769Z, mohini.halingale@ncs.co, {NCS Group, 9/5/2025 0:00, , , https://careers.in.ncs-i.com/ncsi/jobview/senior-data-engineer-pune-maharashtra-india-2025082010193956?source=linkedin, , , Senior Data Engineer, }, , , 655250561, false, , recipient}            |{1451417, campaign, 2025_08_Ninad_Recruiter_Reachout, NULL}|{80708321700, {3976843, message, 3976840, , follow-up}, sent-message, campaign-message}                    |\n",
      "|open  |33905990004|2025-09-09T09:45:37+0000|true       |{389605000, 2025-09-05T11:27:21.769Z, mohini.halingale@ncs.co, {NCS Group, 9/5/2025 0:00, , , https://careers.in.ncs-i.com/ncsi/jobview/senior-data-engineer-pune-maharashtra-india-2025082010193956?source=linkedin, , , Senior Data Engineer, }, , , 655250561, false, , recipient}            |{1451417, campaign, 2025_08_Ninad_Recruiter_Reachout, NULL}|{80666484100, {3976840, message, NULL, Experienced Data Engineer, initial}, sent-message, campaign-message}|\n",
      "+------+-----------+------------------------+-----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row_count = df.count()\n",
    "print(f\"Total rows: {row_count}\")\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f52f3c7f-85bb-4c71-a327-d8ff913a0e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 3\n",
      "+------+-----------+------------------------+-----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------+---------------------------------------------------------------------------------------+\n",
      "|object|id         |actionDate              |isDuplicate|recipient                                                                                                                                                                                                                                |campaign                                                            |parent                                                                                 |\n",
      "+------+-----------+------------------------+-----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------+---------------------------------------------------------------------------------------+\n",
      "|open  |34580550004|2026-01-02T22:42:13+0000|true       |{397579444, 2025-12-09T13:56:16.833Z, austin.nix@qgenda.com, {QGenda, 2025-12-09 0:00:00, , , https://job-boards.greenhouse.io/qgenda/jobs/5011920008?gh_src=40f3284f8us, , , Senior Data Engineer, }, , , 669557495, false, , recipient}|{1363166, campaign, 2024_12_Yogita_Nesargi_Recruiter_Reachout, NULL}|{82675471900, {3755784, message, 3755757, , follow-up}, sent-message, campaign-message}|\n",
      "+------+-----------+------------------------+-----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------+---------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.parquet(\"s3a://mailshake-analytics/raw/client_id=client_4/entity=activity_open/event_date=2026-01-02/activity_open_20260102T231425.parquet\")\n",
    "print(f\"Total rows: {row_count}\")\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4fa060c-6d6b-4a6f-a63a-d48af9224692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 4\n",
      "+--------+-------+--------------------+--------------------+----------+--------+------------+--------------------+--------------------+-------------+-------------------+--------------------+---------------+--------------------+\n",
      "|  object|     id|               title|             created|isArchived|isPaused|wizardStatus|            messages|                 url|sender.object|          sender.id| sender.emailAddress|sender.fromName|      sender.created|\n",
      "+--------+-------+--------------------+--------------------+----------+--------+------------+--------------------+--------------------+-------------+-------------------+--------------------+---------------+--------------------+\n",
      "|campaign|1369019|Ernest_Recruiter_...|2025-01-10T06:55:...|     false|    true|  InProgress|[{3770927, false,...|https://mailshake...|       sender|143568-157165-false| auernest7@gmail.com|      Ernest Au|2025-01-08T04:44:...|\n",
      "|campaign|1368762|2024_12_Ernest_Au...|2025-01-09T16:31:...|     false|    true|  InProgress|[{3770398, false,...|https://mailshake...|       sender|143568-157165-false| auernest7@gmail.com|      Ernest Au|2025-01-08T04:44:...|\n",
      "|campaign|1363648|2024_12_Saikrishn...|2024-12-18T16:57:...|     false|    true|  InProgress|[{3757241, false,...|https://mailshake...|       sender|142724-156315-false|saikrishna.bondal...|   Saikrishna B|2024-12-18T16:56:...|\n",
      "|campaign|1363166|2024_12_Yogita_Ne...|2024-12-17T14:40:...|     false|   false|  InProgress|[{3755757, false,...|https://mailshake...|       sender|142607-156197-false| yogitasne@gmail.com| Yogita Nesargi|2024-12-17T08:19:...|\n",
      "+--------+-------+--------------------+--------------------+----------+--------+------------+--------------------+--------------------+-------------+-------------------+--------------------+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.parquet(\"s3a://mailshake-analytics/raw/client_id=client_1/entity=campaign/date=2025-12-31/part-00000.parquet\")\n",
    "row_count = df2.count()\n",
    "print(f\"Total rows: {row_count}\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "006eab68-a0e3-4a53-96d4-26710a3871a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------------------+--------------------+----------+--------+------------+--------------------+--------------------+-------------+-------------------+--------------------+--------------------+--------------------+\n",
      "|  object|     id|               title|             created|isArchived|isPaused|wizardStatus|            messages|                 url|sender.object|          sender.id| sender.emailAddress|     sender.fromName|      sender.created|\n",
      "+--------+-------+--------------------+--------------------+----------+--------+------------+--------------------+--------------------+-------------+-------------------+--------------------+--------------------+--------------------+\n",
      "|campaign|1488098|2025_12_Sanghamit...|2025-12-19T14:32:...|     false|   false|  InProgress|[{4065551, false,...|https://mailshake...|       sender|164486-178439-false|palsanghamitra22@...|     Sanghamitra Pal|2025-12-19T14:23:...|\n",
      "|campaign|1488093|2025_12_Vansh_Gup...|2025-12-19T14:12:...|     false|   false|  InProgress|[{4065536, false,...|https://mailshake...|       sender|164485-178438-false|vanshguptainca@gm...|         Vansh Gupta|2025-12-19T14:11:...|\n",
      "|campaign|1458402|2025_09_Avinash_P...|2025-09-09T08:10:...|     false|    true|  InProgress|[{3994937, false,...|https://mailshake...|       sender|159285-173136-false|pashamavinashredd...|Avinash Reddy Pasham|2025-09-09T08:09:...|\n",
      "|campaign|1447546|Cold_Emailing_Meg...|2025-08-06T10:11:...|     false|   false|  InProgress|[{3967985, false,...|https://mailshake...|       sender|157092-170911-false|megha.mh10@gmail.com|          Megha Hole|2025-08-06T09:34:...|\n",
      "|campaign|1447134|2025_08_Lekhya_Sa...|2025-08-05T11:22:...|     false|    true|  InProgress|[{3966984, false,...|https://mailshake...|       sender|157006-170827-false|sakelekhya@gmail.com|         Lekhya Sake|2025-08-05T11:13:...|\n",
      "|campaign|1425545|02_06_2025_Recrui...|2025-06-02T15:28:...|     false|    true|  InProgress|[{3914469, false,...|https://mailshake...|       sender|153577-167366-false|dhyeydeacademy@gm...|         Dhyey Patel|2025-06-02T15:26:...|\n",
      "|campaign|1425327|2025_06_Megha_hol...|2025-06-02T09:50:...|     false|   false|  InProgress|[{3914483, false,...|https://mailshake...|       sender|153567-167356-false|meghahole109@gmai...|          Megha Hole|2025-06-02T09:49:...|\n",
      "|campaign|1419805|          Brian Rowe|2025-05-19T15:28:...|     false|    true|  InProgress|[{3900330, false,...|https://mailshake...|       sender|152559-166330-false| browe7379@gmail.com|              B Rowe|2025-05-19T15:26:...|\n",
      "|campaign|1410676|2025_04_McLain_Cr...|2025-04-23T13:58:...|     false|    true|  InProgress|[{3876605, false,...|https://mailshake...|       sender|150635-164379-false|mclaincronin.mc@g...|       McLain Cronin|2025-04-17T10:14:...|\n",
      "+--------+-------+--------------------+--------------------+----------+--------+------------+--------------------+--------------------+-------------+-------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55ecef5-5035-44aa-9ba7-253a8ffeadd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
