{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31ff4bfb-6012-4c21-9ad8-fa190135011b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12.4 MB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.9.0.post0)\n",
      "Collecting numpy>=1.20.3; python_version < \"3.10\"\n",
      "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17.3 MB 22 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2025.2)\n",
      "Collecting tzdata>=2022.1\n",
      "  Downloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 348 kB 3.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Installing collected packages: numpy, tzdata, pandas\n",
      "Successfully installed numpy-1.24.4 pandas-2.0.3 tzdata-2025.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1285345f-ce9c-4e9f-8eb6-4161fcf849c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading boto3-1.37.38-py3-none-any.whl (139 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 139 kB 5.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3transfer<0.12.0,>=0.11.0\n",
      "  Downloading s3transfer-0.11.5-py3-none-any.whl (84 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 84 kB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting botocore<1.38.0,>=1.37.38\n",
      "  Downloading botocore-1.37.38-py3-none-any.whl (13.5 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13.5 MB 5.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<1.27,>=1.25.4; python_version < \"3.10\"\n",
      "  Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 144 kB 5.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.38.0,>=1.37.38->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.38->boto3) (1.17.0)\n",
      "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.3\n",
      "    Uninstalling urllib3-2.2.3:\n",
      "      Successfully uninstalled urllib3-2.2.3\n",
      "Successfully installed boto3-1.37.38 botocore-1.37.38 jmespath-1.0.1 s3transfer-0.11.5 urllib3-1.26.20\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dea47a8d-3089-4987-80c3-c2153efb337a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-17.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.0 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40.0 MB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from pyarrow) (1.24.4)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-17.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff2015bb-f99c-4902-b765-91abf099f2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teams to process: 1\n",
      "\n",
      "üöÄ Team 111158\n",
      "üîÑ INCREMENTAL RUN\n",
      "‚úì campaign: 4 records (FULL)\n",
      "‚úì activity_sent: 20 records (INCREMENTAL)\n",
      "‚úì activity_open: 7 records (INCREMENTAL)\n",
      "‚úì created_leads: 50 records (FULL)\n",
      "‚úÖ Team 111158 completed\n",
      "\n",
      "üèÅ JOB FINISHED\n",
      "Total API calls: 11\n",
      "Total time: 0.3 min\n",
      "Avg rate: 39.8 calls/min\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from typing import Dict, List, Any\n",
    "import boto3\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================================\n",
    "# GLOBALS\n",
    "# ============================================================================\n",
    "BASE = \"https://api.mailshake.com/2017-04-01\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "BUCKET = \"mailshake-analytics\"\n",
    "RAW_PREFIX = \"raw\"\n",
    "WATERMARK_PREFIX = \"metadata/watermark\"\n",
    "TEAMS_KEY = \"config/teams_test3_sample.json\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "RUN_DATE = datetime.utcnow().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "API_CALL_DELAY_FIRST_RUN = 3.0\n",
    "API_CALL_DELAY_INCREMENTAL = 1.0\n",
    "\n",
    "MAX_PAGES_FIRST_RUN = 100\n",
    "MAX_PAGES_INCREMENTAL = 3\n",
    "\n",
    "is_first_run_mode = False\n",
    "api_call_count = 0\n",
    "start_time = None\n",
    "\n",
    "# ============================================================================\n",
    "# TEAM CONFIG\n",
    "# ============================================================================\n",
    "def load_teams() -> Dict[str, Dict[str, str]]:\n",
    "    obj = s3.get_object(Bucket=BUCKET, Key=TEAMS_KEY)\n",
    "    return json.loads(obj[\"Body\"].read().decode(\"utf-8\")).get(\"teams\", {})\n",
    "\n",
    "# ============================================================================\n",
    "# SAFE POST\n",
    "# ============================================================================\n",
    "def safe_post(url: str, payload: Dict[str, Any], auth: HTTPBasicAuth) -> requests.Response:\n",
    "    global api_call_count, start_time\n",
    "\n",
    "    if start_time is None:\n",
    "        start_time = time.time()\n",
    "\n",
    "    delay = API_CALL_DELAY_FIRST_RUN if is_first_run_mode else API_CALL_DELAY_INCREMENTAL\n",
    "    time.sleep(delay)\n",
    "\n",
    "    api_call_count += 1\n",
    "    elapsed = time.time() - start_time\n",
    "    rate = api_call_count / (elapsed / 60) if elapsed > 0 else 0\n",
    "\n",
    "    resp = requests.post(url, json=payload, headers=HEADERS, auth=auth, timeout=30)\n",
    "\n",
    "    if resp.status_code == 429:\n",
    "        endpoint = url.split(\"/\")[-1]\n",
    "        mode = \"FIRST RUN\" if is_first_run_mode else \"INCREMENTAL\"\n",
    "        print(\"\\nüö´ RATE LIMIT HIT\")\n",
    "        print(f\"Endpoint: {endpoint}\")\n",
    "        print(f\"Mode: {mode}\")\n",
    "        print(f\"Rate: {rate:.1f} calls/min\")\n",
    "        raise SystemExit(1)\n",
    "\n",
    "    resp.raise_for_status()\n",
    "    return resp\n",
    "\n",
    "# ============================================================================\n",
    "# WATERMARK HELPERS\n",
    "# ============================================================================\n",
    "def read_watermarks(team_id: str) -> dict:\n",
    "    key = f\"{WATERMARK_PREFIX}/watermark_{team_id}.json\"\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=BUCKET, Key=key)\n",
    "        return json.loads(obj[\"Body\"].read().decode(\"utf-8\"))\n",
    "    except s3.exceptions.NoSuchKey:\n",
    "        return {}\n",
    "\n",
    "def update_watermarks(team_id: str, new_data: dict):\n",
    "    key = f\"{WATERMARK_PREFIX}/watermark_{team_id}.json\"\n",
    "    current = read_watermarks(team_id)\n",
    "    current.update(new_data)\n",
    "    s3.put_object(Bucket=BUCKET, Key=key, Body=json.dumps(current, indent=2).encode(\"utf-8\"))\n",
    "\n",
    "def get_watermark(team_id: str, entity: str) -> str:\n",
    "    return read_watermarks(team_id).get(entity, \"1970-01-01T00:00:00Z\")\n",
    "\n",
    "def normalize_iso(ts: str) -> str:\n",
    "    if not ts:\n",
    "        return ts\n",
    "    if ts.endswith(\"Z\"):\n",
    "        return ts.replace(\"Z\", \"+00:00\")\n",
    "    if len(ts) >= 5:\n",
    "        tz = ts[-5:]\n",
    "        if tz[0] in \"+-\" and tz[1:].isdigit() and \":\" not in tz:\n",
    "            return ts[:-5] + tz[:3] + \":\" + tz[3:]\n",
    "    return ts\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE PARQUET\n",
    "# ============================================================================\n",
    "def save_snapshot_or_incremental(\n",
    "    data: list, team_id: str, entity: str, ts_col: str, first_run: bool\n",
    "):\n",
    "    if not data:\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    if ts_col in df.columns:\n",
    "        df[ts_col] = pd.to_datetime(df[ts_col], errors=\"coerce\", utc=True)\n",
    "        df = df.dropna(subset=[ts_col])\n",
    "        if df.empty:\n",
    "            return None\n",
    "        df[ts_col] = df[ts_col].dt.strftime(\"%Y-%m-%dT%H:%M:%S+00:00\")\n",
    "\n",
    "    batch_ts = datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%S+00:00\")\n",
    "\n",
    "    # FULL\n",
    "    if entity in [\"campaign\", \"created_leads\"]:\n",
    "        file = f\"/tmp/{team_id}_{entity}_full.parquet\"\n",
    "        key = f\"{RAW_PREFIX}/team_id={team_id}/entity={entity}/snapshot/{entity}.parquet\"\n",
    "        df.to_parquet(file, index=False)\n",
    "        s3.upload_file(file, BUCKET, key)\n",
    "        print(f\"‚úì {entity}: {len(df)} records (FULL)\")\n",
    "        return df[ts_col].max() if ts_col in df.columns else batch_ts\n",
    "\n",
    "    # FIRST RUN SNAPSHOT\n",
    "    if first_run and entity.startswith(\"activity_\"):\n",
    "        file = f\"/tmp/{team_id}_{entity}_snapshot.parquet\"\n",
    "        key = f\"{RAW_PREFIX}/team_id={team_id}/entity={entity}/snapshot/{entity}.parquet\"\n",
    "        df.to_parquet(file, index=False)\n",
    "        s3.upload_file(file, BUCKET, key)\n",
    "        print(f\"‚úì {entity}: {len(df)} records (SNAPSHOT 20d)\")\n",
    "        return df[ts_col].max()\n",
    "\n",
    "    # INCREMENTAL\n",
    "    current_wm = get_watermark(team_id, entity)\n",
    "    if ts_col in df.columns:\n",
    "        df = df[df[ts_col] > current_wm]\n",
    "    if df.empty:\n",
    "        return None\n",
    "\n",
    "    df[\"event_date\"] = pd.to_datetime(df[ts_col], utc=True).dt.strftime(\"%Y-%m-%d\")\n",
    "    new_wm = df[ts_col].max()\n",
    "\n",
    "    total = 0\n",
    "    for d in df[\"event_date\"].unique():\n",
    "        part = df[df[\"event_date\"] == d].drop(columns=[\"event_date\"])\n",
    "        suffix = new_wm.replace(\":\", \"\").replace(\"-\", \"\")\n",
    "        file = f\"/tmp/{team_id}_{entity}_{d}_{suffix}.parquet\"\n",
    "        key = f\"{RAW_PREFIX}/team_id={team_id}/entity={entity}/event_date={d}/{entity}_{suffix}.parquet\"\n",
    "        part.to_parquet(file, index=False)\n",
    "        s3.upload_file(file, BUCKET, key)\n",
    "        total += len(part)\n",
    "\n",
    "    print(f\"‚úì {entity}: {total} records (INCREMENTAL)\")\n",
    "    return new_wm\n",
    "\n",
    "# ============================================================================\n",
    "# FETCH FUNCTIONS\n",
    "# ============================================================================\n",
    "def fetch_campaigns(team_id: str, auth: HTTPBasicAuth) -> List[Dict[str, Any]]:\n",
    "    resp = safe_post(f\"{BASE}/campaigns/list\", {\"teamID\": team_id}, auth)\n",
    "    return resp.json().get(\"results\", []) or []\n",
    "\n",
    "def fetch_activity_with_since(\n",
    "    team_id: str, api_name: str, since_ts: str, auth: HTTPBasicAuth, first_run: bool\n",
    ") -> List[Dict[str, Any]]:\n",
    "    payload = {\"teamID\": team_id, \"perPage\": 100}\n",
    "\n",
    "    if first_run:\n",
    "        cutoff = datetime.now(timezone.utc) - timedelta(days=20)\n",
    "        payload[\"since\"] = cutoff.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        max_pages = MAX_PAGES_FIRST_RUN\n",
    "    elif since_ts and since_ts != \"1970-01-01T00:00:00Z\":\n",
    "        since_dt = datetime.fromisoformat(normalize_iso(since_ts))\n",
    "        payload[\"since\"] = since_dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        max_pages = MAX_PAGES_INCREMENTAL\n",
    "    else:\n",
    "        max_pages = MAX_PAGES_FIRST_RUN\n",
    "\n",
    "    results_all = []\n",
    "    page = 0\n",
    "\n",
    "    while True:\n",
    "        page += 1\n",
    "        if page > max_pages:\n",
    "            break\n",
    "\n",
    "        resp = safe_post(f\"{BASE}/activity/{api_name}\", payload, auth)\n",
    "        data = resp.json()\n",
    "        results = data.get(\"results\", [])\n",
    "        if not results:\n",
    "            break\n",
    "\n",
    "        results_all.extend(results)\n",
    "        token = data.get(\"nextToken\")\n",
    "        if not token:\n",
    "            break\n",
    "\n",
    "        payload = {\"teamID\": team_id, \"nextToken\": token, \"perPage\": 100}\n",
    "        if \"since\" in payload:\n",
    "            payload[\"since\"] = payload.get(\"since\")\n",
    "\n",
    "    return results_all\n",
    "\n",
    "def fetch_sent_activity(\n",
    "    team_id: str, watermark: str, first_run: bool, auth: HTTPBasicAuth\n",
    ") -> List[Dict[str, Any]]:\n",
    "    payload = {\"teamID\": team_id, \"perPage\": 100}\n",
    "    cutoff = (\n",
    "        datetime.now(timezone.utc) - timedelta(days=20)\n",
    "        if first_run\n",
    "        else datetime.fromisoformat(normalize_iso(watermark))\n",
    "    )\n",
    "\n",
    "    results_all = []\n",
    "    page = 0\n",
    "\n",
    "    while True:\n",
    "        page += 1\n",
    "        if page > (MAX_PAGES_FIRST_RUN if first_run else MAX_PAGES_INCREMENTAL):\n",
    "            break\n",
    "\n",
    "        resp = safe_post(f\"{BASE}/activity/sent\", payload, auth)\n",
    "        data = resp.json()\n",
    "        results = data.get(\"results\", [])\n",
    "        if not results:\n",
    "            break\n",
    "\n",
    "        stop = False\n",
    "        for r in results:\n",
    "            ts = r.get(\"actionDate\")\n",
    "            if not ts:\n",
    "                continue\n",
    "            try:\n",
    "                dt = datetime.fromisoformat(normalize_iso(ts))\n",
    "            except:\n",
    "                continue\n",
    "            if dt >= cutoff:\n",
    "                results_all.append(r)\n",
    "            else:\n",
    "                stop = True\n",
    "\n",
    "        if stop and not first_run:\n",
    "            break\n",
    "\n",
    "        token = data.get(\"nextToken\")\n",
    "        if not token:\n",
    "            break\n",
    "\n",
    "        payload = {\"teamID\": team_id, \"nextToken\": token, \"perPage\": 100}\n",
    "\n",
    "    return results_all\n",
    "\n",
    "# ============================================================================\n",
    "# RUN TEAM\n",
    "# ============================================================================\n",
    "def run_team(team: Dict[str, str]):\n",
    "    global is_first_run_mode\n",
    "\n",
    "    team_id, auth = team[\"team_id\"], HTTPBasicAuth(team[\"api_token\"], \"\")\n",
    "    print(f\"\\nüöÄ Team {team_id}\")\n",
    "\n",
    "    watermarks = read_watermarks(team_id)\n",
    "    is_first_run_mode = len(watermarks) == 0\n",
    "    print(\"üÜï FIRST RUN\" if is_first_run_mode else \"üîÑ INCREMENTAL RUN\")\n",
    "\n",
    "    campaigns = fetch_campaigns(team_id, auth)\n",
    "    if campaigns:\n",
    "        wm = save_snapshot_or_incremental(campaigns, team_id, \"campaign\", \"created\", True)\n",
    "        if wm:\n",
    "            update_watermarks(team_id, {\"campaign\": wm})\n",
    "\n",
    "    activity_config = {\n",
    "        \"activity_sent\": (\"sent\", \"actionDate\", False),\n",
    "        \"activity_open\": (\"opens\", \"actionDate\", True),\n",
    "        \"activity_reply\": (\"replies\", \"actionDate\", True),\n",
    "        \"activity_clicks\": (\"clicks\", \"actionDate\", True),\n",
    "        \"created_leads\": (\"created-leads\", \"created\", True),\n",
    "    }\n",
    "\n",
    "    watermarks = read_watermarks(team_id)\n",
    "\n",
    "    for entity, (api, ts_col, supports_since) in activity_config.items():\n",
    "        first_run = entity not in watermarks\n",
    "        watermark = watermarks.get(entity, \"1970-01-01T00:00:00Z\")\n",
    "\n",
    "        data = (\n",
    "            fetch_activity_with_since(team_id, api, watermark, auth, first_run)\n",
    "            if supports_since\n",
    "            else fetch_sent_activity(team_id, watermark, first_run, auth)\n",
    "        )\n",
    "\n",
    "        if not data:\n",
    "            continue\n",
    "\n",
    "        new_wm = save_snapshot_or_incremental(data, team_id, entity, ts_col, first_run)\n",
    "        if new_wm:\n",
    "            update_watermarks(team_id, {entity: new_wm})\n",
    "\n",
    "    print(f\"‚úÖ Team {team_id} completed\")\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    teams = load_teams()\n",
    "\n",
    "    print(f\"Teams to process: {len(teams)}\")\n",
    "\n",
    "    for _, team_cfg in teams.items():\n",
    "        try:\n",
    "            run_team(team_cfg)\n",
    "        except SystemExit:\n",
    "            print(\"üö´ RATE LIMIT ‚Äî STOPPING\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Team failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "    if start_time:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\nüèÅ JOB FINISHED\")\n",
    "        print(f\"Total API calls: {api_call_count}\")\n",
    "        print(f\"Total time: {elapsed/60:.1f} min\")\n",
    "        print(f\"Avg rate: {api_call_count/(elapsed/60):.1f} calls/min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "496d688d-40b8-42c4-8076-db8ef5ea83d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-17 02:04:31,797 [INFO] Teams to process: 1\n",
      "2026-02-17 02:04:31,800 [INFO] üöÄ Team 97949\n",
      "2026-02-17 02:04:31,899 [INFO] üîÑ INCREMENTAL RUN\n",
      "2026-02-17 02:04:33,408 [INFO] ‚úì campaign: 15 records (FULL)\n",
      "2026-02-17 02:04:39,092 [INFO] ‚úì activity_sent: 75 records (INCREMENTAL)\n",
      "2026-02-17 02:04:44,662 [INFO] ‚úì activity_open: 36 records (INCREMENTAL)\n",
      "2026-02-17 02:04:50,884 [INFO] ‚úì activity_reply: 16 records (INCREMENTAL)\n",
      "2026-02-17 02:04:55,113 [INFO] ‚úì activity_clicks: 2 records (INCREMENTAL)\n",
      "2026-02-17 02:04:59,224 [INFO] ‚úì created_leads: 220 records (FULL)\n",
      "2026-02-17 02:04:59,386 [INFO] ‚úÖ Team 97949 completed\n",
      "2026-02-17 02:04:59,390 [INFO] üèÅ JOB FINISHED\n",
      "2026-02-17 02:04:59,396 [INFO] Total API calls: 16\n",
      "2026-02-17 02:04:59,400 [INFO] Total time: 0.5 min\n",
      "2026-02-17 02:04:59,405 [INFO] Avg rate: 34.9 calls/min\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from typing import Dict, List, Any\n",
    "import boto3\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================================\n",
    "# LOGGING CONFIG\n",
    "# ============================================================================\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ============================================================================\n",
    "# GLOBALS\n",
    "# ============================================================================\n",
    "BASE = \"https://api.mailshake.com/2017-04-01\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "BUCKET = \"mailshake-analytics\"\n",
    "RAW_PREFIX = \"raw\"\n",
    "WATERMARK_PREFIX = \"metadata/watermark\"\n",
    "TEAMS_KEY = \"config/teams_test2_sample.json\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "RUN_DATE = datetime.utcnow().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "API_CALL_DELAY_FIRST_RUN = 3.0\n",
    "API_CALL_DELAY_INCREMENTAL = 1.0\n",
    "\n",
    "MAX_PAGES_FIRST_RUN = 100\n",
    "MAX_PAGES_INCREMENTAL = 3\n",
    "\n",
    "is_first_run_mode = False\n",
    "api_call_count = 0\n",
    "start_time = None\n",
    "\n",
    "# ============================================================================\n",
    "# TEAM CONFIG\n",
    "# ============================================================================\n",
    "def load_teams() -> Dict[str, Dict[str, str]]:\n",
    "    obj = s3.get_object(Bucket=BUCKET, Key=TEAMS_KEY)\n",
    "    return json.loads(obj[\"Body\"].read().decode(\"utf-8\")).get(\"teams\", {})\n",
    "\n",
    "# ============================================================================\n",
    "# SAFE POST\n",
    "# ============================================================================\n",
    "def safe_post(url: str, payload: Dict[str, Any], auth: HTTPBasicAuth) -> requests.Response:\n",
    "    global api_call_count, start_time\n",
    "\n",
    "    if start_time is None:\n",
    "        start_time = time.time()\n",
    "\n",
    "    delay = API_CALL_DELAY_FIRST_RUN if is_first_run_mode else API_CALL_DELAY_INCREMENTAL\n",
    "    time.sleep(delay)\n",
    "\n",
    "    api_call_count += 1\n",
    "    elapsed = time.time() - start_time\n",
    "    rate = api_call_count / (elapsed / 60) if elapsed > 0 else 0\n",
    "\n",
    "    resp = requests.post(url, json=payload, headers=HEADERS, auth=auth, timeout=30)\n",
    "\n",
    "    if resp.status_code == 429:\n",
    "        endpoint = url.split(\"/\")[-1]\n",
    "        mode = \"FIRST RUN\" if is_first_run_mode else \"INCREMENTAL\"\n",
    "        logger.error(\"üö´ RATE LIMIT HIT\")\n",
    "        logger.error(f\"Endpoint: {endpoint}\")\n",
    "        logger.error(f\"Mode: {mode}\")\n",
    "        logger.error(f\"Rate: {rate:.1f} calls/min\")\n",
    "        raise SystemExit(1)\n",
    "\n",
    "    resp.raise_for_status()\n",
    "    return resp\n",
    "\n",
    "# ============================================================================\n",
    "# WATERMARK HELPERS\n",
    "# ============================================================================\n",
    "def read_watermarks(team_id: str) -> dict:\n",
    "    key = f\"{WATERMARK_PREFIX}/watermark_{team_id}.json\"\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=BUCKET, Key=key)\n",
    "        return json.loads(obj[\"Body\"].read().decode(\"utf-8\"))\n",
    "    except s3.exceptions.NoSuchKey:\n",
    "        return {}\n",
    "\n",
    "def update_watermarks(team_id: str, new_data: dict):\n",
    "    key = f\"{WATERMARK_PREFIX}/watermark_{team_id}.json\"\n",
    "    current = read_watermarks(team_id)\n",
    "    current.update(new_data)\n",
    "    s3.put_object(Bucket=BUCKET, Key=key, Body=json.dumps(current, indent=2).encode(\"utf-8\"))\n",
    "\n",
    "def get_watermark(team_id: str, entity: str) -> str:\n",
    "    return read_watermarks(team_id).get(entity, \"1970-01-01T00:00:00Z\")\n",
    "\n",
    "def normalize_iso(ts: str) -> str:\n",
    "    if not ts:\n",
    "        return ts\n",
    "    if ts.endswith(\"Z\"):\n",
    "        return ts.replace(\"Z\", \"+00:00\")\n",
    "    if len(ts) >= 5:\n",
    "        tz = ts[-5:]\n",
    "        if tz[0] in \"+-\" and tz[1:].isdigit() and \":\" not in tz:\n",
    "            return ts[:-5] + tz[:3] + \":\" + tz[3:]\n",
    "    return ts\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE PARQUET\n",
    "# ============================================================================\n",
    "def save_snapshot_or_incremental(\n",
    "    data: list, team_id: str, entity: str, ts_col: str, first_run: bool\n",
    "):\n",
    "    if not data:\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    if ts_col in df.columns:\n",
    "        df[ts_col] = pd.to_datetime(df[ts_col], errors=\"coerce\", utc=True)\n",
    "        df = df.dropna(subset=[ts_col])\n",
    "        if df.empty:\n",
    "            return None\n",
    "        df[ts_col] = df[ts_col].dt.strftime(\"%Y-%m-%dT%H:%M:%S+00:00\")\n",
    "\n",
    "    batch_ts = datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%S+00:00\")\n",
    "\n",
    "    # FULL\n",
    "    if entity in [\"campaign\", \"created_leads\"]:\n",
    "        file = f\"/tmp/{team_id}_{entity}_full.parquet\"\n",
    "        key = f\"{RAW_PREFIX}/team_id={team_id}/entity={entity}/snapshot/{entity}.parquet\"\n",
    "        df.to_parquet(file, index=False)\n",
    "        s3.upload_file(file, BUCKET, key)\n",
    "        logger.info(f\"‚úì {entity}: {len(df)} records (FULL)\")\n",
    "        return df[ts_col].max() if ts_col in df.columns else batch_ts\n",
    "\n",
    "    # FIRST RUN SNAPSHOT\n",
    "    if first_run and entity.startswith(\"activity_\"):\n",
    "        file = f\"/tmp/{team_id}_{entity}_snapshot.parquet\"\n",
    "        key = f\"{RAW_PREFIX}/team_id={team_id}/entity={entity}/snapshot/{entity}.parquet\"\n",
    "        df.to_parquet(file, index=False)\n",
    "        s3.upload_file(file, BUCKET, key)\n",
    "        logger.info(f\"‚úì {entity}: {len(df)} records (SNAPSHOT 20d)\")\n",
    "        return df[ts_col].max()\n",
    "\n",
    "    # INCREMENTAL\n",
    "    current_wm = get_watermark(team_id, entity)\n",
    "    if ts_col in df.columns:\n",
    "        df = df[df[ts_col] > current_wm]\n",
    "    if df.empty:\n",
    "        return None\n",
    "\n",
    "    df[\"event_date\"] = pd.to_datetime(df[ts_col], utc=True).dt.strftime(\"%Y-%m-%d\")\n",
    "    new_wm = df[ts_col].max()\n",
    "\n",
    "    total = 0\n",
    "    for d in df[\"event_date\"].unique():\n",
    "        part = df[df[\"event_date\"] == d].drop(columns=[\"event_date\"])\n",
    "        suffix = new_wm.replace(\":\", \"\").replace(\"-\", \"\")\n",
    "        file = f\"/tmp/{team_id}_{entity}_{d}_{suffix}.parquet\"\n",
    "        key = f\"{RAW_PREFIX}/team_id={team_id}/entity={entity}/event_date={d}/{entity}_{suffix}.parquet\"\n",
    "        part.to_parquet(file, index=False)\n",
    "        s3.upload_file(file, BUCKET, key)\n",
    "        total += len(part)\n",
    "\n",
    "    logger.info(f\"‚úì {entity}: {total} records (INCREMENTAL)\")\n",
    "    return new_wm\n",
    "\n",
    "# ============================================================================\n",
    "# FETCH FUNCTIONS\n",
    "# ============================================================================\n",
    "def fetch_campaigns(team_id: str, auth: HTTPBasicAuth) -> List[Dict[str, Any]]:\n",
    "    resp = safe_post(f\"{BASE}/campaigns/list\", {\"teamID\": team_id}, auth)\n",
    "    return resp.json().get(\"results\", []) or []\n",
    "\n",
    "def fetch_activity_with_since(\n",
    "    team_id: str, api_name: str, since_ts: str, auth: HTTPBasicAuth, first_run: bool\n",
    ") -> List[Dict[str, Any]]:\n",
    "    payload = {\"teamID\": team_id, \"perPage\": 100}\n",
    "\n",
    "    if first_run:\n",
    "        cutoff = datetime.now(timezone.utc) - timedelta(days=20)\n",
    "        payload[\"since\"] = cutoff.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        max_pages = MAX_PAGES_FIRST_RUN\n",
    "    elif since_ts and since_ts != \"1970-01-01T00:00:00Z\":\n",
    "        since_dt = datetime.fromisoformat(normalize_iso(since_ts))\n",
    "        payload[\"since\"] = since_dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        max_pages = MAX_PAGES_INCREMENTAL\n",
    "    else:\n",
    "        max_pages = MAX_PAGES_FIRST_RUN\n",
    "\n",
    "    results_all = []\n",
    "    page = 0\n",
    "\n",
    "    while True:\n",
    "        page += 1\n",
    "        if page > max_pages:\n",
    "            break\n",
    "\n",
    "        resp = safe_post(f\"{BASE}/activity/{api_name}\", payload, auth)\n",
    "        data = resp.json()\n",
    "        results = data.get(\"results\", [])\n",
    "        if not results:\n",
    "            break\n",
    "\n",
    "        results_all.extend(results)\n",
    "        token = data.get(\"nextToken\")\n",
    "        if not token:\n",
    "            break\n",
    "\n",
    "        payload = {\"teamID\": team_id, \"nextToken\": token, \"perPage\": 100}\n",
    "\n",
    "    return results_all\n",
    "\n",
    "def fetch_sent_activity(\n",
    "    team_id: str, watermark: str, first_run: bool, auth: HTTPBasicAuth\n",
    ") -> List[Dict[str, Any]]:\n",
    "    payload = {\"teamID\": team_id, \"perPage\": 100}\n",
    "    cutoff = (\n",
    "        datetime.now(timezone.utc) - timedelta(days=20)\n",
    "        if first_run\n",
    "        else datetime.fromisoformat(normalize_iso(watermark))\n",
    "    )\n",
    "\n",
    "    results_all = []\n",
    "    page = 0\n",
    "\n",
    "    while True:\n",
    "        page += 1\n",
    "        if page > (MAX_PAGES_FIRST_RUN if first_run else MAX_PAGES_INCREMENTAL):\n",
    "            break\n",
    "\n",
    "        resp = safe_post(f\"{BASE}/activity/sent\", payload, auth)\n",
    "        data = resp.json()\n",
    "        results = data.get(\"results\", [])\n",
    "        if not results:\n",
    "            break\n",
    "\n",
    "        stop = False\n",
    "        for r in results:\n",
    "            ts = r.get(\"actionDate\")\n",
    "            if not ts:\n",
    "                continue\n",
    "            try:\n",
    "                dt = datetime.fromisoformat(normalize_iso(ts))\n",
    "            except Exception:\n",
    "                continue\n",
    "            if dt >= cutoff:\n",
    "                results_all.append(r)\n",
    "            else:\n",
    "                stop = True\n",
    "\n",
    "        if stop and not first_run:\n",
    "            break\n",
    "\n",
    "        token = data.get(\"nextToken\")\n",
    "        if not token:\n",
    "            break\n",
    "\n",
    "        payload = {\"teamID\": team_id, \"nextToken\": token, \"perPage\": 100}\n",
    "\n",
    "    return results_all\n",
    "\n",
    "# ============================================================================\n",
    "# RUN TEAM\n",
    "# ============================================================================\n",
    "def run_team(team: Dict[str, str]):\n",
    "    global is_first_run_mode\n",
    "\n",
    "    team_id = team[\"team_id\"]\n",
    "    auth = HTTPBasicAuth(team[\"api_token\"], \"\")\n",
    "    logger.info(f\"üöÄ Team {team_id}\")\n",
    "\n",
    "    watermarks = read_watermarks(team_id)\n",
    "    is_first_run_mode = len(watermarks) == 0\n",
    "    logger.info(\"üÜï FIRST RUN\" if is_first_run_mode else \"üîÑ INCREMENTAL RUN\")\n",
    "\n",
    "    campaigns = fetch_campaigns(team_id, auth)\n",
    "    if campaigns:\n",
    "        wm = save_snapshot_or_incremental(campaigns, team_id, \"campaign\", \"created\", True)\n",
    "        if wm:\n",
    "            update_watermarks(team_id, {\"campaign\": wm})\n",
    "\n",
    "    activity_config = {\n",
    "        \"activity_sent\": (\"sent\", \"actionDate\", False),\n",
    "        \"activity_open\": (\"opens\", \"actionDate\", True),\n",
    "        \"activity_reply\": (\"replies\", \"actionDate\", True),\n",
    "        \"activity_clicks\": (\"clicks\", \"actionDate\", True),\n",
    "        \"created_leads\": (\"created-leads\", \"created\", True),\n",
    "    }\n",
    "\n",
    "    watermarks = read_watermarks(team_id)\n",
    "\n",
    "    for entity, (api, ts_col, supports_since) in activity_config.items():\n",
    "        first_run = entity not in watermarks\n",
    "        watermark = watermarks.get(entity, \"1970-01-01T00:00:00Z\")\n",
    "\n",
    "        data = (\n",
    "            fetch_activity_with_since(team_id, api, watermark, auth, first_run)\n",
    "            if supports_since\n",
    "            else fetch_sent_activity(team_id, watermark, first_run, auth)\n",
    "        )\n",
    "\n",
    "        if not data:\n",
    "            continue\n",
    "\n",
    "        new_wm = save_snapshot_or_incremental(data, team_id, entity, ts_col, first_run)\n",
    "        if new_wm:\n",
    "            update_watermarks(team_id, {entity: new_wm})\n",
    "\n",
    "    logger.info(f\"‚úÖ Team {team_id} completed\")\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    teams = load_teams()\n",
    "\n",
    "    logger.info(f\"Teams to process: {len(teams)}\")\n",
    "\n",
    "    for _, team_cfg in teams.items():\n",
    "        try:\n",
    "            run_team(team_cfg)\n",
    "        except SystemExit:\n",
    "            logger.error(\"üö´ RATE LIMIT ‚Äî STOPPING\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"‚ùå Team failed: {e}\")\n",
    "\n",
    "    if start_time:\n",
    "        elapsed = time.time() - start_time\n",
    "        logger.info(\"üèÅ JOB FINISHED\")\n",
    "        logger.info(f\"Total API calls: {api_call_count}\")\n",
    "        logger.info(f\"Total time: {elapsed/60:.1f} min\")\n",
    "        logger.info(f\"Avg rate: {api_call_count/(elapsed/60):.1f} calls/min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d16532-cbe2-444c-83b9-58c5617f8d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
